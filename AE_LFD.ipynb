{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VAE_LFD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPEIE+6DAjlzem+Y+SQeJWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunsinghrathore/Subject-Independent-Emotion-Recognition/blob/main/AE_LFD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF_49fEx-zT2",
        "outputId": "c8a16d94-b74c-4a52-8b8e-da974add1405"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun  7 05:29:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G47XZ52FvAsT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H63uWTyIFAwo"
      },
      "source": [
        "# **AE TRAINING IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbrSKjSNFEB3"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 500\n",
        "original_dim = 32\n",
        "epochs = 15\n",
        "zscore = True\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "    for latent_dim in range(128,129):\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "        # encoder\n",
        "        eeg_input = Input(shape=(original_dim,))\n",
        "        dense1 = Dense(latent_dim, activation='sigmoid')(eeg_input)\n",
        "        encoder = Model(eeg_input, dense1)\n",
        "        # decoder\n",
        "        eeg_output = Dense(original_dim)(dense1)\n",
        "\n",
        "        ae = Model(eeg_input, eeg_output)\n",
        "        ae.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "        x_test = x_train\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        ae.fit(x_train, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = ae.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/AE/encoded_eegs_1ae/encoded_eegs_1ae_sub' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VAE/encoded_eegs_2vae_z_sub/encoded_eegs_2vae_z_sub' +\n",
        "        #             str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #             {'encoded_eegs_z': encoded_x_z})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        #sio.savemat('D:\\\\VAE Experiment\\\\DEAP\\\\decoded_eegs_2vae\\\\decoded_eegs_2vae_sub' +\n",
        "        #            str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #            {'decoded_eegs': decoded_x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dViy7ewxYfOt"
      },
      "source": [
        "# **DEEP AE IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oe_mUY0YeSS"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 500\n",
        "original_dim = 32\n",
        "epochs = 30\n",
        "zscore = True\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "    for latent_dim in range(32,33):\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "        # encoder\n",
        "        eeg_input = Input(shape=(original_dim,))\n",
        "        encoded = Dense(latent_dim*4, activation='sigmoid')(eeg_input)\n",
        "        encoded = Dense(latent_dim*2, activation='sigmoid')(encoded)\n",
        "        dense1 = Dense(latent_dim, activation='sigmoid')(encoded)\n",
        "        encoder = Model(eeg_input, dense1)\n",
        "        # decoder\n",
        "        decoded = Dense(latent_dim*2, activation='sigmoid')(dense1)\n",
        "        decoded = Dense(latent_dim*4, activation='sigmoid')(decoded)\n",
        "        eeg_output = Dense(original_dim)(decoded)\n",
        "\n",
        "        ae = Model(eeg_input, eeg_output)\n",
        "        ae.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "        x_test = x_train\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        ae.fit(x_train, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = ae.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DAE/encoded_eegs_1Dae/encoded_eegs_1Dae_sub' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VAE/encoded_eegs_2vae_z_sub/encoded_eegs_2vae_z_sub' +\n",
        "        #             str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #             {'encoded_eegs_z': encoded_x_z})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        #sio.savemat('D:\\\\VAE Experiment\\\\DEAP\\\\decoded_eegs_2vae\\\\decoded_eegs_2vae_sub' +\n",
        "        #            str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #            {'decoded_eegs': decoded_x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXp_yZVgkxc0"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=32\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pqeTbqAkxc4"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DAE/encoded_eegs_1Dae/encoded_eegs_1Dae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DAE_ecoded_dataset_32.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acC1wH-5b3OW"
      },
      "source": [
        "# **Convolutional 1D AE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KbE4tO2Ou_0"
      },
      "source": [
        "# from tqdm import tqdm\n",
        "# import numpy as np\n",
        "# from keras.layers import Input, Dense, LeakyReLU, Conv1D, MaxPooling1D, UpSampling1D, Reshape, Flatten, AveragePooling1D\n",
        "# from keras import layers\n",
        "# from keras.models import Model\n",
        "# import scipy.io as sio\n",
        "# import h5py\n",
        "# import tensorflow as tf\n",
        "\n",
        "# batch_size = 500\n",
        "# original_dim = 32\n",
        "# epochs = 30\n",
        "# zscore = True\n",
        "# latent_dim = 16\n",
        "\n",
        "# input_window = Input(shape=(original_dim,1))\n",
        "# print(input_window.shape)\n",
        "# x = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(input_window) # 10 dims\n",
        "# #x = BatchNormalization()(x)\n",
        "# x = MaxPooling1D(2, padding=\"same\")(x) # 5 dims\n",
        "# x = Conv1D(1, 3, activation=\"relu\", padding=\"same\")(x) # 5 dims\n",
        "# #x = BatchNormalization()(x)\n",
        "# encoded = MaxPooling1D(1, padding=\"same\")(x) # 3 dims\n",
        "# print(encoded.shape)\n",
        "# encoded = AveragePooling1D()(encoded)\n",
        "# print(encoded.shape)\n",
        "# encoded = attention()(encoded) # this is added\n",
        "\n",
        "# encoded = Flatten()(encoded)\n",
        "# encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "# print(encoded.shape)\n",
        "# encoder = Model(input_window, encoded)\n",
        "\n",
        "# # 3 dimensions in the encoded layer\n",
        "# x = Dense(latent_dim)(encoded)\n",
        "# x = Reshape((16,1))(x)\n",
        "# x = Conv1D(1, 3, activation=\"relu\", padding=\"same\")(x) # 3 dims\n",
        "# #x = BatchNormalization()(x)\n",
        "# x = UpSampling1D(2)(x) # 6 dims\n",
        "# x = Conv1D(16, 2, activation='relu', padding=\"same\")(x) # 5 dims\n",
        "# #x = BatchNormalization()(x)\n",
        "# x = UpSampling1D(1)(x) # 10 dims\n",
        "# decoded = Conv1D(1, 3, activation='sigmoid', padding='same')(x) # 10 dims\n",
        "# print(decoded.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iASTMSg0QHc0"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H3nfHwaFoRL"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajpXUfXYb8C1"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Reshape, AveragePooling1D, BatchNormalization\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 500\n",
        "original_dim = 32\n",
        "epochs = 15\n",
        "zscore = True\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "    for latent_dim in range(8,9):\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "        input_window = Input(shape=(original_dim,1))\n",
        "        x = Conv1D(16, 3, activation=\"relu\", padding=\"same\")(input_window) # 10 dims\n",
        "        x = BatchNormalization()(x)\n",
        "        x = MaxPooling1D(2, padding=\"same\")(x) # 5 dims\n",
        "        x = Conv1D(1, 3, activation=\"relu\", padding=\"same\")(x) # 5 dims\n",
        "        x = BatchNormalization()(x)\n",
        "        encoded = MaxPooling1D(2, padding=\"same\")(x) # 3 dims\n",
        "        # encoded = AveragePooling1D()(encoded)\n",
        "        #encoded = attention()(encoded) # this is added\n",
        "\n",
        "        encoded = Flatten()(encoded)\n",
        "        encoded = Dense(latent_dim, activation='softmax')(encoded)\n",
        "\n",
        "        encoder = Model(input_window, encoded)\n",
        "\n",
        "        # 3 dimensions in the encoded layer\n",
        "        x = Dense(latent_dim)(encoded)\n",
        "        x = Reshape((8,1))(x)\n",
        "        x = Conv1D(1, 3, activation=\"relu\", padding=\"same\")(x) # 3 dims\n",
        "        x = BatchNormalization()(x)\n",
        "        x = UpSampling1D(2)(x) # 6 dims\n",
        "        x = Conv1D(16, 2, activation='relu', padding=\"same\")(x) # 5 dims\n",
        "        x = BatchNormalization()(x)\n",
        "        x = UpSampling1D(2)(x) # 10 dims\n",
        "        decoded = Conv1D(1, 3, activation='sigmoid', padding='same')(x) # 10 dims\n",
        "        \n",
        "        ae = Model(input_window, decoded)\n",
        "        ae.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,32)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,32,1)\n",
        "        x_train_noise = x_train_noise.reshape(-1,32,1)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        ae.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = ae.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "        encoded_x = encoded_x.reshape(-1,latent_dim)\n",
        "\n",
        "        print('encoded_x : ',encoded_x.shape)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/CNNAE/encoded_eegs_1CNNae/encoded_eegs_1CNNae_sub2222' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VAE/encoded_eegs_2vae_z_sub/encoded_eegs_2vae_z_sub' +\n",
        "        #             str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #             {'encoded_eegs_z': encoded_x_z})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        #sio.savemat('D:\\\\VAE Experiment\\\\DEAP\\\\decoded_eegs_2vae\\\\decoded_eegs_2vae_sub' +\n",
        "        #            str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #            {'decoded_eegs': decoded_x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj1sr8AXk-4n"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UCXgD3Sk-4n"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/CNNAE/encoded_eegs_1CNNae/encoded_eegs_1CNNae_sub2222'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DCNNAE_ecoded_dataset_8888.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyjjFU3vjpf2"
      },
      "source": [
        "# **Contractive AE IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lRAA40-jtpE"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "batch_size = 500\n",
        "original_dim = 32\n",
        "epochs = 15\n",
        "zscore = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "    for latent_dim in range(32,33):\n",
        "\n",
        "        lam = 1e-4\n",
        "\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "        # encoder\n",
        "        eeg_input = Input(shape=(original_dim,))\n",
        "        dense1 = Dense(latent_dim, activation='sigmoid', name='encoded')(eeg_input)\n",
        "        encoder = Model(eeg_input, dense1)\n",
        "        # decoder\n",
        "        eeg_output = Dense(original_dim)(dense1)\n",
        "        \n",
        "\n",
        "        ae = Model(eeg_input, eeg_output)\n",
        "\n",
        "        # def contractive_loss(y_pred, y_true):\n",
        "        #       mse = K.mean(K.square(y_true - y_pred), axis=1)\n",
        "\n",
        "        #       W = K.variable(value=ae.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
        "        #       W = K.transpose(W)  # N_hidden x N\n",
        "        #       h = ae.get_layer('encoded').output\n",
        "        #       dh = h * (1 - h)  # N_batch x N_hidden\n",
        "\n",
        "        #       # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
        "        #       contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
        "\n",
        "        #       return mse + contractive\n",
        "\n",
        "\n",
        "        mse = K.mean(K.square(eeg_input - eeg_output), axis=1)\n",
        "        W = K.variable(value=ae.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
        "        W = K.transpose(W)  # N_hidden x N\n",
        "        h = ae.get_layer('encoded').output\n",
        "\n",
        "        dh = h * (1 - h)  # N_batch x N_hidden\n",
        "\n",
        "        # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
        "        contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
        "\n",
        "        ae_loss = mse + contractive\n",
        "        ae.add_loss(ae_loss)\n",
        "        \n",
        "\n",
        "        ae.compile(optimizer='adam')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "        x_test = x_train\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        ae.fit(x_train, x_train,\n",
        "                #shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = ae.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/CAE/encoded_eegs_1Cae/encoded_eegs_1Cae_sub' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VAE/encoded_eegs_2vae_z_sub/encoded_eegs_2vae_z_sub' +\n",
        "        #             str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #             {'encoded_eegs_z': encoded_x_z})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        #sio.savemat('D:\\\\VAE Experiment\\\\DEAP\\\\decoded_eegs_2vae\\\\decoded_eegs_2vae_sub' +\n",
        "        #            str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #            {'decoded_eegs': decoded_x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIs_cw6leYT5"
      },
      "source": [
        "# **Seq-Seq LSTM AE IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhaHeWv8edIs"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 32\n",
        "epochs = 20\n",
        "zscore = True\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "\n",
        "        timesteps = 1  # Length of your sequences\n",
        "        input_dim = 32 \n",
        "        latent_dim = 16\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        encoded = LSTM(latent_dim)(inputs)\n",
        "\n",
        "        decoded = RepeatVector(timesteps)(encoded)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "        x_train = x_train.reshape(-1,1,32)\n",
        "        x_test = x_train\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        sequence_autoencoder.fit(x_train, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/SEQAE/encoded_eegs_1SEQae/encoded_eegs_1SEQae_sub' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VAE/encoded_eegs_2vae_z_sub/encoded_eegs_2vae_z_sub' +\n",
        "        #             str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #             {'encoded_eegs_z': encoded_x_z})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        #sio.savemat('D:\\\\VAE Experiment\\\\DEAP\\\\decoded_eegs_2vae\\\\decoded_eegs_2vae_sub' +\n",
        "        #            str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #            {'decoded_eegs': decoded_x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1NpE6Q9rpIX"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=16\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeoz_CtKrpIb"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/SEQAE/encoded_eegs_1SEQae/encoded_eegs_1SEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/SEQAE_ecoded_dataset_16.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dw4R0a1y70r"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xuB0-_k3JA6"
      },
      "source": [
        "# **DeNoising Seq-Sec AE IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iPBS4OJ3P7N"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVS0fT2b3U-G"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 32\n",
        "epochs = 25\n",
        "zscore = True\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "\n",
        "        timesteps = 1  # Length of your sequences\n",
        "        input_dim = 32 \n",
        "        latent_dim = 8\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        encoded = LSTM(latent_dim*2)(inputs)\n",
        "        encoded = LSTM(latent_dim)(encoded)\n",
        "\n",
        "        # decoded = RepeatVector(timesteps)(encoded)\n",
        "        decoded = LSTM(latent_dim*2)(decoded)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "        # inputs = Input(shape=(timesteps, input_dim))\n",
        "        # encoded = LSTM(latent_dim, activation=\"relu\", return_sequences=False)(inputs)\n",
        "        # #encoded = attention()(encoded) # this is added\n",
        "\n",
        "\n",
        "        # decoded = RepeatVector(timesteps, name=\"bottleneck_output\")(encoded)\n",
        "        # decoded = LSTM(latent_dim, activation=\"relu\", return_sequences=True)(decoded)\n",
        "        # decoded = TimeDistributed(Dense(input_dim))(decoded)\n",
        "\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,32)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,1,32)\n",
        "        x_train_noise = x_train_noise.reshape(-1,1,32)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888_' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub' +\n",
        "        #         str(subNo) + '_latentdim' + str(Config['latent_dim']) + '.mat',\n",
        "        #          {'decoded_eegs': decoded_x.reshape(-1,32)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpASwzRS3U-H"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPkf2DJd3U-H"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888_'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nz.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stef-a7lTmsT"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L28ICOsME_Ex"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['decoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_decoded_dataset_64.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guq0qvgFrnGC"
      },
      "source": [
        "# **DeNoising Seq-Sec AE IND T**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U63NIm35rnGF"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqKi_uzV77ov"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer, LeakyReLU, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Reshape, GlobalAveragePooling1D, Bidirectional\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wshbRF07i4bO"
      },
      "source": [
        "# from tqdm import tqdm\n",
        "# import numpy as np\n",
        "# from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector\n",
        "# from keras.models import Model\n",
        "# import scipy.io as sio\n",
        "# import h5py\n",
        "# import tensorflow as tf\n",
        "# import keras.backend as K\n",
        "\n",
        "# batch_size = 512\n",
        "# original_dim = 32\n",
        "# zscore = True\n",
        "\n",
        "# X_train = []\n",
        "\n",
        "# for subNo in tqdm(range(1,33)): #subNo 从1到32\n",
        "#         #### train the VAE on normalized (z-score) multi-channel EEG data\n",
        "#         print('subNo: '+str(subNo))\n",
        "#         if zscore:\n",
        "#             sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "#             x_train = sub_data_file['zscore_data']\n",
        "#         else:\n",
        "#             sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "#             x_train = sub_data_file['minmax_data']\n",
        "\n",
        "#         print('shape : ',x_train.shape)\n",
        "#         X_train.append(np.transpose(x_train)[:, 0:32])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr1vQkcrjElz"
      },
      "source": [
        "# X_train = np.array(X_train).reshape(-1,8064,32)\n",
        "# X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnRZPZIFrnGG"
      },
      "source": [
        "batch_size = 40\n",
        "original_dim = 32\n",
        "epochs = 600\n",
        "zscore = True\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,2): #subNo 从1到32\n",
        "\n",
        "        timesteps = 8064  # Length of your sequences\n",
        "        input_dim = 32 \n",
        "        latent_dim = 8\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        encoded = LSTM(latent_dim, return_sequences=True)(inputs)\n",
        "        encoded = attention()(tf.transpose(encoded, perm = [0,2,1]))\n",
        "        encoded = tf.transpose(encoded, perm = [0,2,1])\n",
        "        \n",
        "        decoded = LSTM(input_dim, return_sequences=True)(encoded)\n",
        "        \n",
        "\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "        x_train = x_train.reshape(-1,8064,32)\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,8064,32)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,8064,32)\n",
        "        x_train_noise = x_train_noise.reshape(-1,8064,32)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "        callbacks = [es]\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                verbose = 1,\n",
        "                validation_data=(x_test_noise, x_test),\n",
        "                callbacks = callbacks)\n",
        "\n",
        "        #build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "\n",
        "        print('encoded shape : ', encoded_x.shape)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888z_7680nrrrr_c_attn44_bino' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "\n",
        "\n",
        "        # print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3s05jUPZTcm7"
      },
      "source": [
        " sequence_autoencoder.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDNwTgNtjtqO"
      },
      "source": [
        "## **Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEar1R19BwGC"
      },
      "source": [
        "!pip install mne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6Saf6swJKMY"
      },
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.manifold import TSNE\n",
        "import mne\n",
        "# from cuml.manifold import TSNE\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlkFrryWUVpg"
      },
      "source": [
        "# units = int(int(sequence_autoencoder.layers[5].trainable_weights[0].shape[1])/4)\n",
        "# print(\"No units: \", units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzQPKbuAe3m3"
      },
      "source": [
        "# W = sequence_autoencoder.layers[5].get_weights()[0]\n",
        "# U = sequence_autoencoder.layers[5].get_weights()[1]\n",
        "# b = sequence_autoencoder.layers[5].get_weights()[2]\n",
        "\n",
        "# W_i = W[:, :units]\n",
        "# W_f = W[:, units: units * 2]\n",
        "# W_c = W[:, units * 2: units * 3]\n",
        "# W_o = W[:, units * 3:]\n",
        "\n",
        "# U_i = U[:, :units]\n",
        "# U_f = U[:, units: units * 2]\n",
        "# U_c = U[:, units * 2: units * 3]\n",
        "# U_o = U[:, units * 3:]\n",
        "\n",
        "# b_i = b[:units]\n",
        "# b_f = b[units: units * 2]\n",
        "# b_c = b[units * 2: units * 3]\n",
        "# b_o = b[units * 3:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6vHKg8zhGyN"
      },
      "source": [
        "# (sorted(W_i[4]+W_f[4]+W_c[4]))[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tsd540uahMHO"
      },
      "source": [
        "# xxx=W_i[4]+W_f[4]+W_c[4]\n",
        "# xxx[8]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UDhbmT-6Y7f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkOI1fyD91G4"
      },
      "source": [
        "sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(10) + '.mat', 'r')\n",
        "x_train = sub_data_file['zscore_data']\n",
        "x_train = np.transpose(x_train)[:, 0:32]\n",
        "x_train = x_train.reshape(-1,8064,32)\n",
        "x_train.shape\n",
        "\n",
        "# import _pickle as cPickle\n",
        "# filename = '/content/drive/My Drive/EMOTION/data_preprocessed_python/s01.dat'\n",
        "# x_train = (cPickle.load(open(filename, 'rb'), encoding='latin1'))['data'][:, :32, :]\n",
        "# x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq3nxMnn6Y_j"
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nzz7680_attn444__l.pkl', 'rb') as filepath:\n",
        "          x_data = pickle.load(filepath)\n",
        "\n",
        "x_data = x_data.reshape(32,40,8064,-1)\n",
        "x_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHrzsxIz7KFd"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "x_data = ZscoreNormalization(x_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs88aM4R_D97"
      },
      "source": [
        "ind = 26"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvXKr3EP9Q_q"
      },
      "source": [
        "xxx = x_data[9, ind]\n",
        "print(xxx.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-uzCDglVl--"
      },
      "source": [
        "from scipy import signal\n",
        "def ave_freq(data, freq = 128):\n",
        "    win = 4 * freq\n",
        "    freqs, psd = signal.welch(data, freq, nperseg=win, scaling='spectrum')\n",
        "    print(psd.shape)\n",
        "    #print(freqs[4:160])\n",
        "    return psd[:,:].mean(1)\n",
        "\n",
        "dataa_lt = ave_freq(np.transpose(xxx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDhyonFgVv90"
      },
      "source": [
        "dataa_lt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIc_UZnIVzaQ"
      },
      "source": [
        "np.argsort(np.abs(dataa_lt))[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d65BWLnA6ZD8"
      },
      "source": [
        "# Write a function that returns the cosine similarity based on the formula above\n",
        "def cosine_similarity(vectorA,vectorB):\n",
        "    return np.dot(vectorA,vectorB)/(np.linalg.norm(vectorA)*np.linalg.norm(vectorB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_0_wlGi6a2m"
      },
      "source": [
        "def most_similar(custom_vector, embeddings_dict):\n",
        "  # We find the cosine similarity values with every other vector in the embeddings dictionary\n",
        "  cosine_values = {i:cosine_similarity(v,custom_vector) for i, v in enumerate(embeddings_dict)}\n",
        "  # We now find the embedding that is closest\n",
        "  closest = sorted(cosine_values.items(),key =lambda item : item[1], reverse=True)\n",
        "\n",
        "  return cosine_values, closest[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HNkLVDW6a7S"
      },
      "source": [
        "X = np.transpose(x_train[ind])\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGomrh1XjoCt"
      },
      "source": [
        "cos_val, clos = most_similar(xxx[:,2], X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEZHPJCjYEle"
      },
      "source": [
        "# array([ 5, 24, 25, 30,  8,  0,  9,  6,  7, 28, 15,  2,  4, 11, 14, 12, 17,\n",
        "#        23, 19,  3,  1, 16, 22, 10, 18, 27, 13, 26, 31, 21, 20, 29])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzvN6h7B92yF"
      },
      "source": [
        "cos_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qi05vnU923R"
      },
      "source": [
        "clos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWPPzpkiFO53"
      },
      "source": [
        "ch_names = ['Fp1'\t\n",
        "        ,'AF3'\n",
        "        ,'F3'\n",
        "        ,'F7'\n",
        "        ,'FC5'\n",
        "        ,'FC1'\n",
        "        ,'C3'\n",
        "        ,'T7'\n",
        "        ,'CP5'\n",
        "        ,'CP1'\n",
        "        ,'P3'\n",
        "        ,'P7'\n",
        "        ,'PO3'\n",
        "        ,'O1'\n",
        "        ,'Oz'\n",
        "        ,'Pz'\n",
        "        ,'Fp2'\n",
        "        ,'AF4'\n",
        "        ,'Fz'\n",
        "        ,'F4'\n",
        "        ,'F8'\n",
        "        ,'FC6'\n",
        "        ,'FC2'\n",
        "        ,'Cz'\n",
        "        ,'C4'\n",
        "        ,'T8'\n",
        "        ,'CP6'\n",
        "        ,'CP2'\n",
        "        ,'P4'\n",
        "        ,'P8'\n",
        "        ,'PO4'\n",
        "        ,'O2']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lESph7GTKCXx"
      },
      "source": [
        "len(ch_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVFHytIrHocv"
      },
      "source": [
        "import _pickle as cPickle\n",
        "filename = '/content/drive/My Drive/EMOTION/data_preprocessed_python/s25.dat'\n",
        "X = (cPickle.load(open(filename, 'rb'), encoding='latin1'))['data'][:, :32, :]\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGR9pRy7XcR6"
      },
      "source": [
        "# X = np.transpose(x_train[ind])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-Q_AdiQB9iY"
      },
      "source": [
        "def mne_object(data, ch_names, freq, events = None):\n",
        "  # create an mne info file with meta data about the EEG\n",
        "  info = mne.create_info(ch_names=list(ch_names), \n",
        "                         sfreq=freq, \n",
        "                         ch_types=['eeg']*data.shape[0])\n",
        "  \n",
        "  # data needs to be in volts rather than in microvolts\n",
        "  data = data*1e-6\n",
        "  # transpose the data\n",
        "  data_T = data#.transpose()\n",
        "  \n",
        "  # create raw mne object\n",
        "  raw = mne.io.RawArray(data_T, info)\n",
        "\n",
        "  if events:\n",
        "    start_times = np.array(events[::2])\n",
        "    end_times = np.array(events[1::2])\n",
        "    anno_length = end_times-start_times\n",
        "    event_name = np.array(['Ictal']*len(anno_length))\n",
        "\n",
        "    raw.set_annotations(mne.Annotations(start_times,\n",
        "                                      anno_length,\n",
        "                                      event_name))\n",
        "\n",
        "  return raw\n",
        "\n",
        "mne_data = mne_object(X[26], ch_names, 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvqbBr6fJbPF"
      },
      "source": [
        "# mne_data.info['ch_names']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6njcSj4GJS0C"
      },
      "source": [
        "# set the standard montage\n",
        "mne_data.set_montage('standard_1020')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KgTGqsPI325"
      },
      "source": [
        "mne_data.plot_sensors(kind='topomap', show_names=True, to_sphere=True);\n",
        "fig = mne_data.plot_sensors(kind='3d', show_names=True, show=False)\n",
        "fig = fig.gca().view_init(azim=70, elev=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpJ73R0gLpSN"
      },
      "source": [
        "mne_data[:][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhhIQaMbKck2"
      },
      "source": [
        "from scipy import signal\n",
        "def ave_freq(data, freq = 128):\n",
        "    win = 4 * freq\n",
        "    freqs, psd = signal.welch(data, freq, nperseg=win, scaling='spectrum')\n",
        "    print(psd.shape)\n",
        "    #print(freqs[4:160])\n",
        "    return psd[:,:].mean(1)\n",
        "\n",
        "dataa = ave_freq(mne_data[:][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJmFpTbDTZxE"
      },
      "source": [
        "np.argsort(np.abs(dataa))[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETNqkTsEOvLr"
      },
      "source": [
        "mne.viz.plot_topomap(dataa,\n",
        "                      mne_data.info,\n",
        "                      show=False,\n",
        "                      sensors=False,\n",
        "                      names=mne_data.info['ch_names'], \n",
        "                      show_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFMdZHH7PAMt"
      },
      "source": [
        "ch_names.index('F7')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3rxJPyWPFZp"
      },
      "source": [
        "ch_names.index('C4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnLNmbYpPJEi"
      },
      "source": [
        "ch_names.index('P3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2mLB3HyPNVK"
      },
      "source": [
        "ch_names.index('P4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le09jF_KQlLQ"
      },
      "source": [
        "print(ch_names[24])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAqzlZhxQlP7"
      },
      "source": [
        "print(ch_names[10])\n",
        "print(ch_names[13])\n",
        "print(ch_names[30:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSLsMa91jzIt"
      },
      "source": [
        "## **Post-Processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddaiIjm9rnGH"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wBM3Cm4rnGI"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888z_7680nrrrr_c_attn44_bino'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = trainSubData.reshape(-1,8)\n",
        "        # print(traineSubData.shape)\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_final.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train), filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLJbQ1k_rnGI"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Ur6PrwrnGI"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['decoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_decoded_dataset_64.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwoGzy7NytZa"
      },
      "source": [
        "# PCA  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3OkoeKqzWE-"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgRgo-a8zWE_"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/pca/encoded_eegs_pca_sub'+ str(trainSubNo) + '_latedtdim' + str(latdim) + '.mat', 'r')\n",
        "        trainSubData = np.array(file2['encoded_eegs'])\n",
        "        trainSubData = trainSubData.reshape(-1,8)\n",
        "        # print(traineSubData.shape)\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_pca.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train), filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1D8T64jX0v3D"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_04j4uhZEPl"
      },
      "source": [
        "# ICA DEAP\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRwLgNxSZEPt"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhBlNuXtZEPu"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/ICA/encoded_eegs_ica_sub'+ str(trainSubNo) + '_latedtdim' + str(latdim) + '.mat', 'r')\n",
        "        trainSubData = np.array(file2['encoded_eegs'])\n",
        "        trainSubData = trainSubData.reshape(-1,8)\n",
        "        # print(traineSubData.shape)\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_ICA.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train), filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKKdkJU8ZEPv"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y94DRIJUqRqk"
      },
      "source": [
        "# **DeNoising Seq-Sec AE IND SEED**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMOFv_iuqRqn"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import os\n",
        "import re\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l33yNb8qzxI"
      },
      "source": [
        "def get_all_purp(root_dir):\n",
        "   all_mat = dict()\n",
        "   for dirName, _, fileList in os.walk(root_dir):\n",
        "       for fname in fileList[:-1]:\n",
        "           #print(fname)\n",
        "           if '.mat' in fname:\n",
        "              sub_no = int(re.split(r'/|_',fname)[-2])\n",
        "              if sub_no in all_mat:\n",
        "                all_exist = all_mat[sub_no]\n",
        "                all_mat[sub_no] = all_exist + [dirName + '/' + fname]\n",
        "              else:\n",
        "                all_mat[sub_no] = [dirName + '/' + fname]\n",
        "   return all_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd6-7Vehq5hT"
      },
      "source": [
        "all_files = get_all_purp('/content/drive/MyDrive/EMOTION/Preprocessed_EEG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOj_V-9Yy8by"
      },
      "source": [
        "all_files[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P38XvdXVqRqo"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "batch_size = 256\n",
        "original_dim = 62\n",
        "epochs = 5\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,16): #subNo 从1到32\n",
        "\n",
        "        timesteps = 1  # Length of your sequences\n",
        "        input_dim = 62 \n",
        "        latent_dim = 16\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        encoded = LSTM(latent_dim)(inputs)\n",
        "\n",
        "        decoded = RepeatVector(timesteps)(encoded)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "        # inputs = Input(shape=(timesteps, input_dim))\n",
        "        # encoded = LSTM(latent_dim, activation=\"relu\", return_sequences=False)(inputs)\n",
        "        # #encoded = attention()(encoded) # this is added\n",
        "\n",
        "\n",
        "        # decoded = RepeatVector(timesteps, name=\"bottleneck_output\")(encoded)\n",
        "        # decoded = LSTM(latent_dim, activation=\"relu\", return_sequences=True)(decoded)\n",
        "        # decoded = TimeDistributed(Dense(input_dim))(decoded)\n",
        "\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        \n",
        "        x_train = np.zeros((15,30000,62))\n",
        "\n",
        "        sub_data_file = []\n",
        "\n",
        "        for filee in all_files[subNo]:\n",
        "          temp = sio.loadmat(filee)\n",
        "          temp = list(temp.values())[3:]\n",
        "          sub_data_file = sub_data_file + temp\n",
        "          break\n",
        "\n",
        "        print('Sub Data File : ',len(sub_data_file))\n",
        "\n",
        "        \n",
        "        for i, dataa in enumerate(sub_data_file):\n",
        "          x_train[i] = np.transpose(ZscoreNormalization(dataa))[1000:31000]\n",
        "\n",
        "        x_train = x_train.reshape(-1,62)\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,62)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,1,62)\n",
        "        x_train_noise = x_train_noise.reshape(-1,1,62)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "\n",
        "        print('Encoded shape : ',encoded_x.shape)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888_seed2' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub' +\n",
        "        #         str(subNo) + '_latentdim' + str(Config['latent_dim']) + '.mat',\n",
        "        #          {'decoded_eegs': decoded_x.reshape(-1,32)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CARYCGExqRqp"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=30000\n",
        "# trialNum=4\n",
        "latdim=16\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJGkMSqTqRqp"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,16):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888_seed2'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 15):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nz_Seed3.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train), filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCiw7mrtqRqq"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOSAUgktqRqr"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['decoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_decoded_dataset_64.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aThiSdddMDr"
      },
      "source": [
        "# **DeNoising Seq-Sec AE IND SEED T**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEesMfgZdMDv"
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "import os\n",
        "import re\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsIrFm-prPF4"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer, LeakyReLU, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Reshape, GlobalAveragePooling1D, Bidirectional\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        # at = 1- at\n",
        "        output=x*at\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMnztcFYdMDv"
      },
      "source": [
        "def get_all_purp(root_dir):\n",
        "   all_mat = dict()\n",
        "   for dirName, _, fileList in os.walk(root_dir):\n",
        "       for fname in fileList[:-1]:\n",
        "           #print(fname)\n",
        "           if '.mat' in fname:\n",
        "              sub_no = int(re.split(r'/|_',fname)[-2])\n",
        "              if sub_no in all_mat:\n",
        "                all_exist = all_mat[sub_no]\n",
        "                all_mat[sub_no] = all_exist + [dirName + '/' + fname]\n",
        "              else:\n",
        "                all_mat[sub_no] = [dirName + '/' + fname]\n",
        "   return all_mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI4voeUGdMDv"
      },
      "source": [
        "all_files = get_all_purp('/content/drive/MyDrive/EMOTION/Preprocessed_EEG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McR64OB-dMDw"
      },
      "source": [
        "all_files[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jm0FtY2dMDw"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "batch_size = 256\n",
        "original_dim = 62\n",
        "epochs = 600\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,16): #subNo 从1到32\n",
        "\n",
        "        timesteps = 30000  # Length of your sequences\n",
        "        input_dim = 62 \n",
        "        latent_dim = 16\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        # encoded = LSTM(latent_dim*2, return_sequences=True)(inputs)\n",
        "        encoded = LSTM(latent_dim, return_sequences=True)(inputs)\n",
        "        encoded = attention()(tf.transpose(encoded, perm = [0,2,1]))\n",
        "        encoded = tf.transpose(encoded, perm = [0,2,1])\n",
        "\n",
        "        # encoded = RepeatVector(timesteps)(encoded)\n",
        "        # decoded = LSTM(latent_dim*2, return_sequences=True)(encoded)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(encoded)\n",
        "\n",
        "        # inputs = Input(shape=(timesteps, input_dim))\n",
        "        # encoded = LSTM(latent_dim, activation=\"relu\", return_sequences=False)(inputs)\n",
        "        # #encoded = attention()(encoded) # this is added\n",
        "\n",
        "\n",
        "        # decoded = RepeatVector(timesteps, name=\"bottleneck_output\")(encoded)\n",
        "        # decoded = LSTM(latent_dim, activation=\"relu\", return_sequences=True)(decoded)\n",
        "        # decoded = TimeDistributed(Dense(input_dim))(decoded)\n",
        "\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        \n",
        "        x_train = np.zeros((15,30000,62))\n",
        "\n",
        "        sub_data_file = []\n",
        "\n",
        "        for filee in all_files[subNo]:\n",
        "          temp = sio.loadmat(filee)\n",
        "          temp = list(temp.values())[3:]\n",
        "          sub_data_file = sub_data_file + temp\n",
        "          break\n",
        "\n",
        "        print('Sub Data File : ',len(sub_data_file))\n",
        "\n",
        "        \n",
        "        for i, dataa in enumerate(sub_data_file):\n",
        "          x_train[i] = np.transpose(ZscoreNormalization(dataa))[1000:31000]\n",
        "\n",
        "        x_train = x_train.reshape(-1,62)\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,62)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,30000,62)\n",
        "        x_train_noise = x_train_noise.reshape(-1,30000,62)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "\n",
        "        print('Encoded shape : ',encoded_x.shape)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888_seed2_attn' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub' +\n",
        "        #         str(subNo) + '_latentdim' + str(Config['latent_dim']) + '.mat',\n",
        "        #          {'decoded_eegs': decoded_x.reshape(-1,32)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu8RMqOAdMDx"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=30000\n",
        "# trialNum=4\n",
        "latdim=16\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8i_HMct6dMDx"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,16):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub888888_seed2_attn'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = trainSubData.reshape(-1,16)\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 15):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nz_Seed3_att2.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train), filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljxfh7NCdMDx"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOXbMxrZ1lMX"
      },
      "source": [
        "X_train[224].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqKG1ourdMDy"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['decoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_decoded_dataset_64.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfXm33A27kA0"
      },
      "source": [
        "# **DeNoising Seq-Sec AE IND Seizure**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7tH0PEB7fu9"
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/x_data_ss.pkl', 'rb') as filepath:\n",
        "      x_data = pickle.load(filepath)\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/y_data_ss.pkl', 'rb') as filepath:\n",
        "      y_data = pickle.load(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s5TCIzI71Os"
      },
      "source": [
        "x_data = x_data.reshape(-1,18*256*20,22)\n",
        "x_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B4glg7g7kA1"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise\n",
        "\n",
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSLbKQqu7kA1"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 22\n",
        "epochs = 25\n",
        "zscore = True\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,12): #subNo 从1到32\n",
        "\n",
        "        timesteps = 1  # Length of your sequences\n",
        "        input_dim = 22\n",
        "        latent_dim = 5\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        encoded = LSTM(latent_dim)(inputs)\n",
        "\n",
        "        decoded = RepeatVector(timesteps)(encoded)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "        # inputs = Input(shape=(timesteps, input_dim))\n",
        "        # encoded = LSTM(latent_dim, activation=\"relu\", return_sequences=False)(inputs)\n",
        "        # #encoded = attention()(encoded) # this is added\n",
        "\n",
        "\n",
        "        # decoded = RepeatVector(timesteps, name=\"bottleneck_output\")(encoded)\n",
        "        # decoded = LSTM(latent_dim, activation=\"relu\", return_sequences=True)(decoded)\n",
        "        # decoded = TimeDistributed(Dense(input_dim))(decoded)\n",
        "\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            x_train = x_data[subNo-1]\n",
        "            x_train = ZscoreNormalization(x_train)\n",
        "\n",
        "        x_train = x_train[:, 0:22]\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,22)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,1,22)\n",
        "        x_train_noise = x_train_noise.reshape(-1,1,22)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub_sss_' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub' +\n",
        "        #         str(subNo) + '_latentdim' + str(Config['latent_dim']) + '.mat',\n",
        "        #          {'decoded_eegs': decoded_x.reshape(-1,32)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGMxxV3T7kA2"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=20*256\n",
        "trialNum=22\n",
        "latdim=5\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqdLP2jj7kA2"
      },
      "source": [
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,12):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub_sss_'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 18):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse_5sss_.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train), filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9Z63DLD7kA3"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FCVDcpO7kA3"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['decoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_decoded_dataset_64.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOmoR7uV7kA3"
      },
      "source": [
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/x_data_s.pkl', 'wb') as filepath:\n",
        "      pickle.dump(x_data, filepath)\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/y_data_s.pkl', 'wb') as filepath:\n",
        "      pickle.dump(y_data, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH7J8TaUdRn1"
      },
      "source": [
        "# **DeNoising Seq-Sec AE IND Seizure T**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOjZlTEldRn2"
      },
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/x_data_ss_ip.pkl', 'rb') as filepath:\n",
        "      x_data = pickle.load(filepath)\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/y_data_ss_ip.pkl', 'rb') as filepath:\n",
        "      y_data = pickle.load(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQazymoQoKHC"
      },
      "source": [
        "y_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QByc4yHONvA"
      },
      "source": [
        "x_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOHpdyZSdRn2"
      },
      "source": [
        "x_data = x_data.reshape(-1,18*256*20,18)\n",
        "x_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcA2eLp2dRn2"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise\n",
        "\n",
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwhPwEIVd-Hx"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer, LeakyReLU, Conv1D, MaxPooling1D, UpSampling1D, Flatten, Reshape, GlobalAveragePooling1D, Bidirectional\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        output=x*at\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YL4cmAtsdRn2"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "# batch_size = 512\n",
        "original_dim = 18\n",
        "epochs = 600\n",
        "zscore = True\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,10): #subNo 从1到32\n",
        "\n",
        "        timesteps = 5120  # Length of your sequences\n",
        "        input_dim = 18\n",
        "        latent_dim = 8\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        # encoded = LSTM(latent_dim*2, return_sequences=True)(inputs)\n",
        "        encoded = LSTM(latent_dim, return_sequences=True)(inputs)\n",
        "        encoded = attention()(tf.transpose(encoded, perm = [0,2,1]))\n",
        "        encoded = tf.transpose(encoded, perm = [0,2,1])\n",
        "\n",
        "        # encoded = RepeatVector(timesteps)(encoded)\n",
        "        # decoded = LSTM(latent_dim*2, return_sequences=True)(encoded)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(encoded)\n",
        "        # inputs = Input(shape=(timesteps, input_dim))\n",
        "        # encoded = LSTM(latent_dim, activation=\"relu\", return_sequences=False)(inputs)\n",
        "        # #encoded = attention()(encoded) # this is added\n",
        "\n",
        "\n",
        "        # decoded = RepeatVector(timesteps, name=\"bottleneck_output\")(encoded)\n",
        "        # decoded = LSTM(latent_dim, activation=\"relu\", return_sequences=True)(decoded)\n",
        "        # decoded = TimeDistributed(Dense(input_dim))(decoded)\n",
        "\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            x_train = x_data[subNo-1]\n",
        "            x_train = ZscoreNormalization(x_train)\n",
        "\n",
        "        x_train = x_train[:, 0:18]\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,18)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,5120,18)\n",
        "        x_train_noise = x_train_noise.reshape(-1,5120,18)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        batch_size = x_train.shape[0]\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "        callbacks = [es]\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test),\n",
        "                callbacks = callbacks)\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "\n",
        "        print('Encoded Shape : ',encoded_x.shape)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub_sss_attn__ip' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub' +\n",
        "        #         str(subNo) + '_latentdim' + str(Config['latent_dim']) + '.mat',\n",
        "        #          {'decoded_eegs': decoded_x.reshape(-1,32)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPQyu1mLdRn4"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=20*256\n",
        "trialNum=22\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8gbjx-SdRn4"
      },
      "source": [
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,10):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub_sss_attn__ip'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = trainSubData.reshape(-1,8)\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 18):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse_5sss_attnn__ip.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train), filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob-r8pvXdRn4"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz9HkBb9dRn4"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['decoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_decoded_dataset_64.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA3aJBmDdRn5"
      },
      "source": [
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/x_data_s.pkl', 'wb') as filepath:\n",
        "      pickle.dump(x_data, filepath)\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/y_data_s.pkl', 'wb') as filepath:\n",
        "      pickle.dump(y_data, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG7D1PvFcZwn"
      },
      "source": [
        "# **DeNoising Seq-Sec AE with ATTn IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9Ea7OucZwn"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-TFYaMqdLu9"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
        "        at=K.softmax(et)\n",
        "        at=K.expand_dims(at,axis=-1)\n",
        "        #at = 1 - at\n",
        "        output=x*at\n",
        "        return K.sum(output,axis=1)\n",
        "\n",
        "    def compute_output_shape(self,input_shape):\n",
        "        return (input_shape[0],input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(attention,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OpRqbF1cZwo"
      },
      "source": [
        "batch_size = 512\n",
        "original_dim = 32\n",
        "epochs = 20\n",
        "zscore = True\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "\n",
        "        timesteps = 1  # Length of your sequences\n",
        "        input_dim = 32 \n",
        "        latent_dim = 8\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        encoded = LSTM(latent_dim, activation=\"relu\", return_sequences=True)(inputs)\n",
        "        #encoded = Dense(latent_dim, activation='relu')(inputs)\n",
        "        encoded = attention()(encoded) # this is added\n",
        "        #encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "\n",
        "        #decoded = Dense(input_dim)(encoded)\n",
        "        decoded = RepeatVector(timesteps, name=\"bottleneck_output\")(encoded)\n",
        "        decoded = LSTM(input_dim, activation=\"sigmoid\", return_sequences=True)(decoded)\n",
        "        \n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "        sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,32)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,1,32)\n",
        "        x_train_noise = x_train_noise.reshape(-1,1,32)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub77' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VAE/encoded_eegs_2vae_z_sub/encoded_eegs_2vae_z_sub' +\n",
        "        #             str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #             {'encoded_eegs_z': encoded_x_z})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        #sio.savemat('D:\\\\VAE Experiment\\\\DEAP\\\\decoded_eegs_2vae\\\\decoded_eegs_2vae_sub' +\n",
        "        #            str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #            {'decoded_eegs': decoded_x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sjS9EiIcZwo"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyTzEMyHcZwo"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub77'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAEA_ecoded_dataset_8mse.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaIKiFnLuIah"
      },
      "source": [
        "np.array(X_train).shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kte8fWAnkpJw"
      },
      "source": [
        "# **Denoising Seq to Seq Model Combined**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThQKYx_mg-VG"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQkophwtkpJ0"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 32\n",
        "zscore = True\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for subNo in tqdm(range(1,33)): #subNo 从1到32\n",
        "        #### train the VAE on normalized (z-score) multi-channel EEG data\n",
        "        print('subNo: '+str(subNo))\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        print('shape : ',x_train.shape)\n",
        "        X_train.append(np.transpose(x_train)[:, 0:32])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnZKGLaPkpJ1"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-5KTyookpJ2"
      },
      "source": [
        "SNR = 1000\n",
        "\n",
        "x_train = np.array(X_train)\n",
        "\n",
        "noise = get_white_noise(x_train.reshape(-1),SNR).reshape(32,-1,32)\n",
        "\n",
        "x_train_noise = np.add(x_train, noise)\n",
        "\n",
        "x_train = x_train.reshape(-1,1,32)\n",
        "x_train_noise = x_train_noise.reshape(-1,1,32)\n",
        "\n",
        "x_test = x_train\n",
        "x_test_noise = x_train_noise\n",
        "\n",
        "print('x_train shape : ',x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8qP6g3kkpJ2"
      },
      "source": [
        "epochs = 30\n",
        "\n",
        "timesteps = 1  # Length of your sequences\n",
        "input_dim = 32 \n",
        "latent_dim = 16\n",
        "lam = 1e-4\n",
        "\n",
        "inputs = Input(shape=(timesteps, input_dim))\n",
        "encoded = LSTM(latent_dim)(inputs)\n",
        "\n",
        "decoded = RepeatVector(timesteps)(encoded)\n",
        "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "sequence_autoencoder = Model(inputs, decoded)\n",
        "encoder = Model(inputs, encoded)\n",
        "\n",
        "sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "        shuffle=True,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test_noise, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1itaeWfkpJ2"
      },
      "source": [
        "x_train_for_decode = x_train.reshape(32,-1,1,32)\n",
        "for subNo, x_train_ind in tqdm(enumerate(x_train_for_decode)):\n",
        "        # build a model to project inputs\n",
        "        encoded_x = encoder.predict(x_train_ind)\n",
        "        \n",
        "\n",
        "        print('encoded_x shape : ',encoded_x.shape)\n",
        "        \n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub' +\n",
        "                    str(subNo+1) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        \n",
        "        decoded_x = sequence_autoencoder.predict(x_train_ind)\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train_ind.reshape(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hEeQNaBkpJ3"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=16\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfdlj7x_kpJ3"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/encoded_eegs_1DSEQae/encoded_eegs_1DSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_all_encoded_dataset_16.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6IvWopAkpJ3"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62ShPWXEBmGU"
      },
      "source": [
        "# **Denoising Seq-Seq VAE IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F0FBleKBu4p"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F-yvTRAEp-t"
      },
      "source": [
        "'''\n",
        " Implementation of a seq-2-seq Variational Autoendocer\n",
        " Inspired by Neural Machine Translation\n",
        " #Reference\n",
        " - Auto-Encoding Variational Bayes\n",
        "   https://arxiv.org/abs/1312.6114\n",
        "'''\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "from scipy.signal import square\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import (Input, Dense, Lambda, Layer, LSTM, Dropout,\n",
        "\t\t\t  TimeDistributed, Embedding, RepeatVector)\n",
        "from keras.callbacks import TensorBoard, Callback\n",
        "from keras.optimizers import RMSprop, Adagrad, Adam\n",
        "from keras.models import load_model, Model\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from keras.losses import mse, binary_crossentropy, mean_squared_error\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "Config = {\n",
        "    'seq_length': 1,\n",
        "    'feature_size': 7680,\n",
        "    'batch_size': 40,\n",
        "    'latent_dim': 1920,\n",
        "    'epochs': 10,\n",
        "    'optimizer': Adam(lr=0.001),\n",
        "\n",
        "    # model architecture\n",
        "    'dense_1': 128,\n",
        "    'dense_2': 64,\n",
        "    'feature_rep_size': 128,  # sequence abstract feature representation\n",
        "}\n",
        "\n",
        "epsilon_std = 1.0\n",
        "\n",
        "\n",
        "main_input = None  # input tensor placeholder\n",
        "\n",
        "\n",
        "def log_dir():\n",
        "    \"\"\"\n",
        "    Inner function of create_model()\n",
        "    Creates unique datetime string for training dir location\n",
        "    \"\"\"\n",
        "    date = datetime.now()\n",
        "    r = '{0}_{1:02d}_{2:02d}_'.format(date.year, date.month, date.day)\n",
        "    r += '{0:02d}_{1:02d}_{2:02d}'.format(date.hour, date.minute, date.second)\n",
        "    return r\n",
        "\n",
        "\n",
        "def roll(x):\n",
        "    shifted = np.zeros_like(x)\n",
        "    shifted[:, :-1, :] = x[0:, 1:, 0:]\n",
        "    return shifted\n",
        "\n",
        "\n",
        "# def SequenceEncoder():\n",
        "#     \"\"\"\n",
        "#     LSTM encoder with the last layer prediction as the input to the dense layer which provides\n",
        "#     variational parameters `guassian mean` and `guassian variance`\n",
        "#     \"\"\"\n",
        "\n",
        "#     global main_input\n",
        "\n",
        "\n",
        "#     main_input = Input(shape=(1, Config['feature_size']))  # LSTM\n",
        "#     seq_1, state_h, state_c = LSTM(Config['feature_rep_size'], return_state=True)(main_input)\n",
        "\n",
        "#     z_mean, z_log_var = (Dense(Config['latent_dim'])(state_h),\n",
        "#                          Dense(Config['latent_dim'])(state_h))\n",
        "#     return z_mean, z_log_var, main_input\n",
        "\n",
        "\n",
        "# def SequenceDecoder(z, generator=False):\n",
        "#     \"\"\"\n",
        "#     LSTM decoder conditioned on the output of the dense layer symetrical to the encoder\n",
        "#     \"\"\"\n",
        "\n",
        "#     hidden_1 = Dense(Config['dense_1'], activation='elu')(z)\n",
        "#     state_1 = Dense(Config['feature_size'])(hidden_1)\n",
        "#     state_2 = Dense(Config['feature_size'])(hidden_1)\n",
        "#     lstm = LSTM(Config['feature_size'], return_sequences=True, activation='sigmoid')(main_input, [state_1, state_2])\n",
        "\n",
        "#     return lstm\n",
        "\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], Config['latent_dim']), mean=0.,\n",
        "                              stddev=epsilon_std)\n",
        "    return z_mean + K.exp(z_log_var) * epsilon\n",
        "\n",
        "\n",
        "# # Custom loss layer\n",
        "# class VariationalLayer(Layer):\n",
        "#     \"\"\"\n",
        "#     `Bottle Neck Layer`. This layer provides the kl-divergence loss function\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, **kwargs):\n",
        "#         self.is_placeholder = True\n",
        "#         self.iter = 0\n",
        "#         self.target = Input(shape=(None, Config['feature_size']))  # original time sequence shift once\n",
        "#         super(VariationalLayer, self).__init__(**kwargs)\n",
        "        \n",
        "\n",
        "#     def call(self, x, x_decoded_mean):\n",
        "#         \"\"\"\n",
        "#         kl-divergence loss plus reconstrucion loss \n",
        "#         \"\"\"\n",
        "\n",
        "#         self.xent_loss = K.mean(metrics.mean_squared_error(self.target, x_decoded_mean), axis=1)  # sum error over time\n",
        "\n",
        "#         self.kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "#         return K.mean(self.xent_loss + self.kl_loss)\n",
        "\n",
        "#     # def call(self, inputs):\n",
        "#     #     x = inputs[0]\n",
        "#     #     x_decoded_mean = inputs[1]\n",
        "#     #     loss = self.vae_loss(x, x_decoded_mean)\n",
        "#     #     self.add_loss(loss, inputs=inputs)\n",
        "#     #     # We won't actually use the output.\n",
        "#     #     return x\n",
        "\n",
        "    \n",
        "\n",
        "class Anealing(Callback):\n",
        "    \"\"\"\n",
        "    scaling the kl-divergence loss by anealing it with a smooth function: tanh in this instance.\n",
        "    the tanh in this case depends on the number of epochs for the the rate of growth.\n",
        "    \"\"\"\n",
        "\n",
        "    def tanh_aneal(self, iter):\n",
        "        return (K.tanh((iter - 5.) / 0.25) + 1.) / 2.\n",
        "\n",
        "    def on_batch_end(self, epoch, logs={}):\n",
        "        return\n",
        "        iter_num = K.tf.cast(self.model.optimizer.iterations, K.tf.float32)\n",
        "        aneal_factor = K.eval(self.tanh_aneal(iter_num))\n",
        "        self.model.get_layer(\"loss_layer\").xent_loss *= aneal_factor\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmoTrn25B0f1"
      },
      "source": [
        "SNR = 1\n",
        "zscore = True\n",
        "\n",
        "\n",
        "\n",
        "for subNo in range(1,33):  \n",
        "\n",
        "      print('subNo: '+str(subNo))\n",
        "\n",
        "      # if zscore:\n",
        "      #     sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "      #     x_train = sub_data_file['zscore_data']\n",
        "      # else:\n",
        "      #     sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "      #     x_train = sub_data_file['minmax_data']\n",
        "\n",
        "\n",
        "\n",
        "      x_train = np.transpose(x_train)[:, 0:32]\n",
        "\n",
        "      noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,32)\n",
        "      #print(noise)\n",
        "      x_train_noise = np.add(x_train, noise)\n",
        "      x_train = x_train.reshape(-1,1,32)\n",
        "      x_train_noise = x_train_noise.reshape(-1,1,32)\n",
        "\n",
        "      x_test = x_train\n",
        "      x_test_noise = x_train_noise\n",
        "\n",
        "      print('x_train shape : ',x_train.shape)\n",
        "      \n",
        "      main_input = Input(shape=(1, Config['feature_size']))  # LSTM\n",
        "      seq_1, state_h, state_c = LSTM(Config['feature_rep_size'], return_state=True)(main_input)\n",
        "\n",
        "      z_mean, z_log_var = (Dense(Config['latent_dim'])(state_h),\n",
        "                          Dense(Config['latent_dim'])(state_h))\n",
        "      # return z_mean, z_log_var, main_input\n",
        "\n",
        "\n",
        "      # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
        "      z = Lambda(sampling, output_shape=(Config['latent_dim'],))([z_mean, z_log_var])\n",
        "\n",
        "      # we instantiate these layers separately so as to reuse them later\n",
        "      hidden_1 = Dense(Config['dense_1'], activation='elu')(z)\n",
        "      state_1 = Dense(Config['feature_size'])(hidden_1)\n",
        "      state_2 = Dense(Config['feature_size'])(hidden_1)\n",
        "      lstm = LSTM(Config['feature_size'], return_sequences=True, activation='sigmoid')(main_input, [state_1, state_2])\n",
        "\n",
        "\n",
        "      # build model graph\n",
        "      # vae_layer = VariationalLayer(name='loss_layer')\n",
        "      #y = vae_layer([x, x_decoded_mean])\n",
        "\n",
        "      # xent_loss = K.mean(binary_crossentropy(x, x_decoded_mean), axis=1)  # sum error over time\n",
        "\n",
        "      # kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "      # vae_loss =  K.mean(xent_loss + kl_loss)\n",
        "\n",
        "      def vae_loss(inputs, outputs):\n",
        "        xent_loss = K.mean(mean_squared_error(inputs, outputs), axis=1)  # sum error over time\n",
        "        kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        vae_loss =  K.mean(xent_loss + kl_loss)\n",
        "        return vae_loss\n",
        "\n",
        "\n",
        "      # compile VAE model\n",
        "      vae = Model(main_input, lstm)\n",
        "      vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
        "\n",
        "\n",
        "      # build encoder and decoder graph nodes\n",
        "      #encoder_layer = Model(main_input, z, name='encoder')\n",
        "\n",
        "      encoder = Model(main_input, z)\n",
        "      decoder = Model(main_input, lstm)\n",
        "\n",
        "      #print(vae.summary())\n",
        "      print(x_train_noise.shape)\n",
        "      print(\"[ INFO ] Ready to train model\")\n",
        "\n",
        "\n",
        "\n",
        "      #shifted_dataset = roll(dataset)\n",
        "      vae.fit(x_train_noise, x_train,\n",
        "            shuffle=True,\n",
        "            epochs=Config['epochs'],\n",
        "            #validation_split=0.1,\n",
        "            batch_size=Config['batch_size'],\n",
        "            validation_data=(x_test_noise, x_test))\n",
        "            #callbacks=[TensorBoard(log_dir='/tmp/dumps/seq_model_' + log_dir()),\n",
        "            #            Anealing()])\n",
        "      # build a model to project inputs\n",
        "      decoded_x = vae.predict(x_train)\n",
        "      encoded_x = encoder.predict(x_train)\n",
        "\n",
        "      print('encoded_x shape : ',encoded_x.shape)\n",
        "      print('decoded_x shape : ',decoded_x.shape)\n",
        "\n",
        "      sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQVAE/encoded_eegs_1DSEQvae/encoded_eegs_1DSEQvae_sub__11' +\n",
        "                str(subNo) + '_latentdim' + str(Config['latent_dim']) + '.mat',\n",
        "                {'encoded_eegs': encoded_x})\n",
        "      \n",
        "\n",
        "      print('Mean Difference : ', np.mean(abs(decoded_x.reshape(-1))-abs(x_train.reshape(-1))))\n",
        "\n",
        "      sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQVAE/decoded_eegs_1DSEQvae/decoded_eegs_1DSEQvae_sub' +\n",
        "                str(subNo) + '_latentdim' + str(Config['latent_dim']) + '.mat',\n",
        "                 {'decoded_eegs': decoded_x.reshape(-1,32)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCKh1YU-G0cX"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7RdGEHQG0cc"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQVAE/encoded_eegs_1DSEQvae/encoded_eegs_1DSEQvae_sub__11_time'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQVAE_ecoded_dataset_8_.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3moiOqV5U58F"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQk9LulZG2hO"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQVAE/decoded_eegs_1DSEQvae/decoded_eegs_1DSEQvae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['decoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQVAE_decoded_dataset_64.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk49RTp3XUfE"
      },
      "source": [
        "sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(10) + '.mat', 'r')\n",
        "x_train = np.array(sub_data_file['zscore_data']).transpose()[:,:32]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6rm2mmZXkba"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygrkBGNVXYa_"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZnim12fXocV"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.array(X_train)[0,500:600,14], color = 'red')\n",
        "plt.plot(x_train[500:600,14])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwLU5F6_d0Dx"
      },
      "source": [
        "# **VQVAE-MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO3xeWDywywO"
      },
      "source": [
        "from __future__ import print_function\n",
        "import abc\n",
        "\n",
        "import numpy as np\n",
        "import logging\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "  \n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Function, Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "\n",
        "class NearestEmbedFunc(Function):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    ------\n",
        "    x - (batch_size, emb_dim, *)\n",
        "        Last dimensions may be arbitrary\n",
        "    emb - (emb_dim, num_emb)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, emb):\n",
        "        if input.size(1) != emb.size(0):\n",
        "            raise RuntimeError('invalid argument: input.size(1) ({}) must be equal to emb.size(0) ({})'.\n",
        "                               format(input.size(1), emb.size(0)))\n",
        "\n",
        "        # save sizes for backward\n",
        "        ctx.batch_size = input.size(0)\n",
        "        ctx.num_latents = int(np.prod(np.array(input.size()[2:])))\n",
        "        ctx.emb_dim = emb.size(0)\n",
        "        ctx.num_emb = emb.size(1)\n",
        "        ctx.input_type = type(input)\n",
        "        ctx.dims = list(range(len(input.size())))\n",
        "\n",
        "        # expand to be broadcast-able\n",
        "        x_expanded = input.unsqueeze(-1)\n",
        "        num_arbitrary_dims = len(ctx.dims) - 2\n",
        "        if num_arbitrary_dims:\n",
        "            emb_expanded = emb.view(emb.shape[0], *([1] * num_arbitrary_dims), emb.shape[1])\n",
        "        else:\n",
        "            emb_expanded = emb\n",
        "\n",
        "        # find nearest neighbors\n",
        "        dist = torch.norm(x_expanded - emb_expanded, 2, 1)\n",
        "        _, argmin = dist.min(-1)\n",
        "        shifted_shape = [input.shape[0], *list(input.shape[2:]) ,input.shape[1]]\n",
        "        result = emb.t().index_select(0, argmin.view(-1)).view(shifted_shape).permute(0, ctx.dims[-1], *ctx.dims[1:-1])\n",
        "\n",
        "        ctx.save_for_backward(argmin)\n",
        "        return result.contiguous(), argmin\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output, argmin=None):\n",
        "        grad_input = grad_emb = None\n",
        "        if ctx.needs_input_grad[0]:\n",
        "            grad_input = grad_output\n",
        "\n",
        "        if ctx.needs_input_grad[1]:\n",
        "            argmin, = ctx.saved_variables\n",
        "            latent_indices = torch.arange(ctx.num_emb).type_as(argmin)\n",
        "            idx_choices = (argmin.view(-1, 1) == latent_indices.view(1, -1)).type_as(grad_output.data)\n",
        "            n_idx_choice = idx_choices.sum(0)\n",
        "            n_idx_choice[n_idx_choice == 0] = 1\n",
        "            idx_avg_choices = idx_choices / n_idx_choice\n",
        "            grad_output = grad_output.permute(0, *ctx.dims[2:], 1).contiguous()\n",
        "            grad_output = grad_output.view(ctx.batch_size * ctx.num_latents, ctx.emb_dim)\n",
        "            grad_emb = torch.sum(grad_output.data.view(-1, ctx.emb_dim, 1) *\n",
        "                                 idx_avg_choices.view(-1, 1, ctx.num_emb), 0)\n",
        "        return grad_input, grad_emb, None, None\n",
        "\n",
        "\n",
        "def nearest_embed(x, emb):\n",
        "    return NearestEmbedFunc().apply(x, emb)\n",
        "\n",
        "\n",
        "class NearestEmbed(nn.Module):\n",
        "    def __init__(self, num_embeddings, embeddings_dim):\n",
        "        super(NearestEmbed, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(embeddings_dim, num_embeddings))\n",
        "\n",
        "    def forward(self, x, weight_sg=False):\n",
        "        \"\"\"Input:\n",
        "        ---------\n",
        "        x - (batch_size, emb_size, *)\n",
        "        \"\"\"\n",
        "        return nearest_embed(x, self.weight.detach() if weight_sg else self.weight)\n",
        "\n",
        "\n",
        "# adapted from https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py#L25\n",
        "# that adapted from https://github.com/deepmind/sonnet\n",
        "\n",
        "\n",
        "class NearestEmbedEMA(nn.Module):\n",
        "    def __init__(self, n_emb, emb_dim, decay=0.99, eps=1e-5):\n",
        "        super(NearestEmbedEMA, self).__init__()\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.embeddings_dim = emb_dim\n",
        "        self.n_emb = n_emb\n",
        "        self.emb_dim = emb_dim\n",
        "        embed = torch.rand(emb_dim, n_emb)\n",
        "        self.register_buffer('weight', embed)\n",
        "        self.register_buffer('cluster_size', torch.zeros(n_emb))\n",
        "        self.register_buffer('embed_avg', embed.clone())\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Input:\n",
        "        ---------\n",
        "        x - (batch_size, emb_size, *)\n",
        "        \"\"\"\n",
        "\n",
        "        dims = list(range(len(x.size())))\n",
        "        x_expanded = x.unsqueeze(-1)\n",
        "        num_arbitrary_dims = len(dims) - 2\n",
        "        if num_arbitrary_dims:\n",
        "            emb_expanded = self.weight.view(self.emb_dim, *([1] * num_arbitrary_dims), self.n_emb)\n",
        "        else:\n",
        "            emb_expanded = self.weight\n",
        "\n",
        "        # find nearest neighbors\n",
        "        dist = torch.norm(x_expanded - emb_expanded, 2, 1)\n",
        "        _, argmin = dist.min(-1)\n",
        "        shifted_shape = [x.shape[0], *list(x.shape[2:]), x.shape[1]]\n",
        "        result = self.weight.t().index_select(0, argmin.view(-1)).view(shifted_shape).permute(0, dims[-1], *dims[1:-1])\n",
        "\n",
        "        if self.training:\n",
        "            latent_indices = torch.arange(self.n_emb).type_as(argmin)\n",
        "            emb_onehot = (argmin.view(-1, 1) == latent_indices.view(1, -1)).type_as(x.data)\n",
        "            n_idx_choice = emb_onehot.sum(0)\n",
        "            n_idx_choice[n_idx_choice == 0] = 1\n",
        "            flatten = x.permute(1, 0, *dims[-2:]).contiguous().view(x.shape[1], -1)\n",
        "\n",
        "            self.cluster_size.data.mul_(self.decay).add_(\n",
        "                1 - self.decay, n_idx_choice\n",
        "            )\n",
        "            embed_sum = flatten @ emb_onehot\n",
        "            self.embed_avg.data.mul_(self.decay).add_(1 - self.decay, embed_sum)\n",
        "\n",
        "            n = self.cluster_size.sum()\n",
        "            cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + self.n_emb * self.eps) * n\n",
        "            )\n",
        "            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n",
        "            self.weight.data.copy_(embed_normalized)\n",
        "\n",
        "        return result, argmin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py7srvOTwxmf"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as tf\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def init_hidden(x: torch.Tensor, hidden_size: int, num_dir: int = 1, xavier: bool = True):\n",
        "    \"\"\"\n",
        "    Initialize hidden.\n",
        "    Args:\n",
        "        x: (torch.Tensor): input tensor\n",
        "        hidden_size: (int):\n",
        "        num_dir: (int): number of directions in LSTM\n",
        "        xavier: (bool): wether or not use xavier initialization\n",
        "    \"\"\"\n",
        "    if xavier:\n",
        "        return nn.init.xavier_normal_(torch.zeros(num_dir, x.size(0), hidden_size)).to(device)\n",
        "    return Variable(torch.zeros(num_dir, x.size(0), hidden_size)).to(device)\n",
        "\n",
        "class VQ_VAE(nn.Module):\n",
        "    \"\"\"Vector Quantized AutoEncoder for mnist\"\"\"\n",
        "    def __init__(self, hidden=8, k=64, vq_coef=0.2, comit_coef=0.4, **kwargs):\n",
        "        super(VQ_VAE, self).__init__()\n",
        "\n",
        "        self.emb_size = k\n",
        "        # self.fc1 = nn.Linear(32, 16)\n",
        "        # self.fc2 = nn.Linear(16, hidden)\n",
        "        # self.lstm = nn.LSTM(input_size=32, hidden_size=hidden)\n",
        "        # self.fc3 = nn.Linear(hidden, 16)\n",
        "        # self.fc4 = nn.Linear(16, 32)\n",
        "        self.lstm_d = nn.LSTM(input_size=hidden, hidden_size=32, bidirectional=False)\n",
        "        self.fc_d = nn.Linear(32, 32)\n",
        "\n",
        "        self.emb = NearestEmbed(k, hidden)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.vq_coef = vq_coef\n",
        "        self.comit_coef = comit_coef\n",
        "        self.hidden = hidden\n",
        "        self.input_size = 32\n",
        "        self.seq_len = 1\n",
        "        self.out_feats = 32\n",
        "        self.encoder_hidden_size = 8\n",
        "        self.decoder_hidden_size = 8\n",
        "        self.mse = 0\n",
        "        self.vq_loss = torch.zeros(1)\n",
        "        self.commit_loss = 0\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.input_size,\n",
        "            hidden_size=self.hidden,\n",
        "            num_layers=1\n",
        "        )\n",
        "        self.attn = nn.Linear(\n",
        "            in_features=2 * self.hidden + self.seq_len,\n",
        "            out_features=1\n",
        "        )\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        # h1 = self.relu(self.fc1(x))\n",
        "        # h2 = self.fc2(h1)\n",
        "\n",
        "        # return h2.view(-1, self.hidden)\n",
        "\n",
        "        x = x.view(-1,1,32)\n",
        "        \n",
        "        h_t, c_t = (init_hidden(x, self.hidden),\n",
        "                    init_hidden(x, self.hidden))\n",
        "        \n",
        "        attentions, input_encoded = (Variable(torch.zeros(x.size(0), self.seq_len, self.input_size)),\n",
        "                                     Variable(torch.zeros(x.size(0), self.seq_len, self.hidden)))\n",
        "      \n",
        "        for t in range(1):\n",
        "            xx = torch.cat((h_t.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
        "                           c_t.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
        "                           x.permute(0, 2, 1).to(device)), dim=2).to(\n",
        "                device)  # bs * input_size * (2 * hidden_dim + seq_len)\n",
        "\n",
        "            e_t = self.attn(xx.view(-1, self.hidden * 2 + self.seq_len))  # (bs * input_size) * 1\n",
        "            a_t = self.softmax(e_t.view(-1, self.input_size)).to(device)  # (bs, input_size)\n",
        "\n",
        "            weighted_input = torch.mul(a_t, x[:, t, :].to(device))  # (bs * input_size)\n",
        "\n",
        "            self.lstm.flatten_parameters()\n",
        "\n",
        "            # print('error : ',self.lstm(x[:, 0, :].unsqueeze(0), (h_t, c_t)))\n",
        "            (h_t, c_t) = self.lstm(weighted_input.unsqueeze(0), (h_t, c_t))\n",
        "            input_encoded[:, t, :] = h_t\n",
        "          \n",
        "        return input_encoded.view(-1, self.hidden).to(device)\n",
        "\n",
        "        \n",
        "\n",
        "    def decode(self, z):\n",
        "        # h3 = self.relu(self.fc3(z))\n",
        "        # return torch.tanh(self.fc4(h3))\n",
        "\n",
        "        z = z.view(-1,1,self.hidden)\n",
        "\n",
        "        h_t, c_t = (init_hidden(z, 32),\n",
        "                    init_hidden(z, 32))\n",
        "        \n",
        "\n",
        "        for t in range(1):\n",
        "            inp = z[:, t].unsqueeze(0)#.unsqueeze(2)\n",
        "            # print('error : ',inp.shape)\n",
        "            lstm_out, (h_t, c_t) = self.lstm_d(inp, (h_t, c_t))\n",
        "        return lstm_out.view(-1, 1, 32)\n",
        "        # # return self.fc_d(lstm_out.squeeze(0)).view(-1, 1, 32)\n",
        "        # return return self.fc_d(lstm_out.view(-1, 32)).view(-1, 1, 32)\n",
        "\n",
        "    def forward(self, x, eval = False):\n",
        "        z_e = self.encode(x.view(-1, 32))\n",
        "        # print('error : ',self.emb(z_e.detach()))\n",
        "        z_q = self.emb(z_e, weight_sg=True)[0].view(-1, self.hidden)\n",
        "        emb = self.emb(z_e.detach())[0].view(-1, self.hidden)\n",
        "        if not(eval):\n",
        "          return self.decode(z_q), z_e, emb\n",
        "        else:\n",
        "          return z_q, self.decode(z_q)\n",
        "\n",
        "    def sample(self, size):\n",
        "        sample = torch.randn(size, self.emb_size, int(self.hidden / self.emb_size))\n",
        "        if self.cuda():\n",
        "            sample = sample.cuda()\n",
        "        emb, _ = self.emb(sample)\n",
        "        sample = self.decode(emb(sample).view(-1, self.hidden)).cpu()\n",
        "        return sample\n",
        "\n",
        "    def loss_function(self, x, recon_x, z_e, emb):\n",
        "        self.mse = F.mse_loss(recon_x, x)\n",
        "\n",
        "        self.vq_loss = torch.mean(torch.norm((emb - z_e.detach())**2, 2, 1))\n",
        "        self.commit_loss = torch.mean(torch.norm((emb.detach() - z_e)**2, 2, 1))\n",
        "\n",
        "        return self.mse + self.vq_coef*self.vq_loss + self.comit_coef*self.commit_loss\n",
        "\n",
        "    def latest_losses(self):\n",
        "        return {'reconst': self.mse, 'vq': self.vq_loss, 'commitment': self.commit_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow0pB6F3yAQX"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4y9LU4KyjBN"
      },
      "source": [
        "def train(epoch, model, train_loader, optimizer, cuda, log_interval):#, save_path, args, writer):\n",
        "    model.train()\n",
        "    loss_dict = model.latest_losses()\n",
        "    losses = {k + '_train': 0 for k, v in loss_dict.items()}\n",
        "    epoch_losses = {k + '_train': 0 for k, v in loss_dict.items()}\n",
        "    start_time = time.time()\n",
        "    batch_idx, data = None, None\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        if cuda:\n",
        "            data = torch.Tensor(data).to('cuda', non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = model.loss_function(data, *outputs)\n",
        "        #print(loss.items())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        latest_losses = model.latest_losses()\n",
        "        for key in latest_losses:\n",
        "            losses[key + '_train'] += float(latest_losses[key])\n",
        "            epoch_losses[key + '_train'] += float(latest_losses[key])\n",
        "\n",
        "        if batch_idx % log_interval == 0:\n",
        "            for key in latest_losses:\n",
        "                losses[key + '_train'] /= log_interval\n",
        "            loss_string = ' '.join(['{}: {:.6f}'.format(k, v) for k, v in losses.items()])\n",
        "            print('Train Epoch: {epoch} [{batch:5d}/{total_batch} ({percent:2d}%)]   time:'\n",
        "                         ' {time:3.2f}   {loss}'\n",
        "                         .format(epoch=epoch, batch=batch_idx * len(data), total_batch=len(train_loader) * len(data),\n",
        "                                 percent=int(100. * batch_idx / len(train_loader)),\n",
        "                                 time=time.time() - start_time,\n",
        "                                 loss=loss_string))\n",
        "            start_time = time.time()\n",
        "            # logging.info('z_e norm: {:.2f}'.format(float(torch.mean(torch.norm(outputs[1][0].contiguous().view(256,-1),2,0)))))\n",
        "            # logging.info('z_q norm: {:.2f}'.format(float(torch.mean(torch.norm(outputs[2][0].contiguous().view(256,-1),2,0)))))\n",
        "            for key in latest_losses:\n",
        "                losses[key + '_train'] = 0\n",
        "        # if batch_idx == (len(train_loader) - 1):\n",
        "        #     save_reconstructed_images(data, epoch, outputs[0], save_path, 'reconstruction_train')\n",
        "\n",
        "        #     write_images(data, outputs, writer, 'train')\n",
        "\n",
        "        # if args.dataset in ['imagenet', 'custom'] and batch_idx * len(data) > args.max_epoch_samples:\n",
        "        #     break\n",
        "\n",
        "    for key in epoch_losses:\n",
        "        # if args.dataset != 'imagenet':\n",
        "        #     epoch_losses[key] /= (len(train_loader.dataset) / train_loader.batch_size)\n",
        "        # else:\n",
        "        epoch_losses[key] /= (len(train_loader) / 128)\n",
        "    # loss_string = '\\t'.join(['{}: {:.6f}'.format(k, v) for k, v in epoch_losses.items()])\n",
        "    # logging.info('====> Epoch: {} {}'.format(epoch, loss_string))\n",
        "    # writer.add_histogram('dict frequency', outputs[3], bins=range(args.k + 1))\n",
        "    # model.print_atom_hist(outputs[3])\n",
        "    return epoch_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkV5OnPSyAQh"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector, TimeDistributed, Layer\n",
        "from keras.models import Model, Sequential\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "# import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "import torch.utils.data\n",
        "from torch import optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.nn import functional as tf\n",
        "\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 32\n",
        "epochs = 15\n",
        "zscore = True\n",
        "SNR = 1\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "\n",
        "        timesteps = 1  # Length of your sequences\n",
        "        input_dim = 32 \n",
        "        latent_dim = 8\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,32)\n",
        "\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,128,1,32)\n",
        "        x_train_noise = x_train_noise.reshape(-1,32)\n",
        "        \n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        model = VQ_VAE(8, k=4).cuda()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, 20, 0.5,)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "          losses = train(epoch, model, x_train, optimizer, cuda = True, log_interval = 256)\n",
        "          print('losses : ',losses)\n",
        "\n",
        "        model.eval()\n",
        "        if True:\n",
        "            data = torch.Tensor(x_train).to('cuda', non_blocking=True)\n",
        "        encoded_x, decoded_x = model(data, True)\n",
        "\n",
        "        print('encoded_x shape : ',encoded_x.shape)\n",
        "        print('decoded_x shape : ',decoded_x.shape)\n",
        "\n",
        "        encoded_x = encoded_x.cpu().detach().numpy()\n",
        "        decoded_x = decoded_x.cpu().detach().numpy()\n",
        "\n",
        "        # build a model to project inputs\n",
        "        # decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        # encoded_x = encoder.predict(x_train)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VQVAE/encoded_eegs_1VQVAEae/encoded_eegs_1VQVAEae_sub64l' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        \n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/DSEQAE/decoded_eegs_1DSEQae/decoded_eegs_1DSEQae_sub' +\n",
        "        #         str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #          {'decoded_eegs': decoded_x.reshape(-1,32)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk8ZH9umFjxA"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHukr366FjxK"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/VQVAE/encoded_eegs_1VQVAEae/encoded_eegs_1VQVAEae_sub64l'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/VQVAE_ecoded_dataset_8_64l.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djfSf-pF9p3F"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okKcHlT7jhKo"
      },
      "source": [
        "# **Seq-Seq LSTM AE Combined**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzfntNUXjmJu"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 32\n",
        "zscore = True\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for subNo in tqdm(range(1,33)): #subNo 从1到32\n",
        "        #### train the VAE on normalized (z-score) multi-channel EEG data\n",
        "        print('subNo: '+str(subNo))\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        print('shape : ',x_train.shape)\n",
        "        X_train.append(np.transpose(x_train)[:, 0:32])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utuZCOhskEZw"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbdxGSjLkEZ5"
      },
      "source": [
        "x_train = np.array(X_train).reshape(-1,1,32)\n",
        "x_test = x_train\n",
        "\n",
        "print('x_train shape : ',x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofoXYNiqjhKp"
      },
      "source": [
        "epochs = 5\n",
        "\n",
        "timesteps = 1  # Length of your sequences\n",
        "input_dim = 32 \n",
        "latent_dim = 32\n",
        "\n",
        "inputs = Input(shape=(timesteps, input_dim))\n",
        "encoded = LSTM(latent_dim)(inputs)\n",
        "\n",
        "decoded = RepeatVector(timesteps)(encoded)\n",
        "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "\n",
        "sequence_autoencoder = Model(inputs, decoded)\n",
        "encoder = Model(inputs, encoded)\n",
        "sequence_autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "sequence_autoencoder.fit(x_train, x_train,\n",
        "        shuffle=True,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOvfMHFSjx89"
      },
      "source": [
        "x_train_for_decode = x_train.reshape(32,-1,1,32)\n",
        "for subNo, x_train_ind in tqdm(enumerate(x_train_for_decode)):\n",
        "        # build a model to project inputs\n",
        "        encoded_x = encoder.predict(x_train_ind)\n",
        "        \n",
        "\n",
        "        print('encoded_x shape : ',encoded_x.shape)\n",
        "        \n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/SEQAE/encoded_eegs_1SEQae/encoded_eegs_1SEQae_sub' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        \n",
        "        decoded_x = sequence_autoencoder.predict(x_train_ind)\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train_ind.reshape(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8opka4a_jhKw"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=32\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yvKMjrvjhKw"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/SEQAE/encoded_eegs_1SEQae/encoded_eegs_1SEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/SEQAE_all_ecoded_dataset_32.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlX-C6Kxpz62"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H8sT1m76tmm"
      },
      "source": [
        "# **Contractive Seq to Seq Model IND**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyxQL0o4oeOc"
      },
      "source": [
        "import math\n",
        "\n",
        "def get_white_noise(signal,SNR) :\n",
        "    #RMS value of signal\n",
        "    RMS_s=math.sqrt(np.mean(signal**2))\n",
        "    #RMS values of noise\n",
        "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
        "    #Additive white gausian noise. Thereore mean=0\n",
        "    #Because sample length is large (typically > 40000)\n",
        "    #we can use the population formula for standard daviation.\n",
        "    #because mean=0 STD=RMS\n",
        "    STD_n=RMS_n\n",
        "    noise=np.random.normal(0, STD_n, signal.shape[0])\n",
        "    return noise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l2MZ6Gl619R"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 32\n",
        "epochs = 30\n",
        "zscore = True\n",
        "SNR = 1000\n",
        "\n",
        "for subNo in range(1,33): #subNo 从1到32\n",
        "\n",
        "        timesteps = 1  # Length of your sequences\n",
        "        input_dim = 32 \n",
        "        latent_dim = 8\n",
        "        lam = 1e-4\n",
        "        print('subNo: '+str(subNo)+' latend_dim: '+str(latent_dim))\n",
        "\n",
        "        inputs = Input(shape=(timesteps, input_dim))\n",
        "        #print(inputs.shape)\n",
        "        encoded = LSTM(latent_dim)(inputs)\n",
        "        encoded = Dense(latent_dim, activation='sigmoid', name='encoded')(encoded)\n",
        "        #print(encoded.shape)\n",
        "        # decoded = Dense(input_dim)(encoded)\n",
        "        decoded = RepeatVector(timesteps)(encoded)\n",
        "        #print(decoded.shape)\n",
        "        decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "        #print(decoded.shape)\n",
        "        sequence_autoencoder = Model(inputs, decoded)\n",
        "        encoder = Model(inputs, encoded)\n",
        "\n",
        "\n",
        "        mse = K.mean(K.square(inputs - decoded), axis=2)\n",
        "        W = K.variable(value=sequence_autoencoder.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
        "        W = K.transpose(W)  # N_hidden x N\n",
        "        h = sequence_autoencoder.get_layer('encoded').output\n",
        "\n",
        "        dh = h * (1 - h)  # N_batch x N_hidden\n",
        "\n",
        "        # N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
        "        contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
        "\n",
        "        ae_loss = mse + contractive\n",
        "        sequence_autoencoder.add_loss(ae_loss)\n",
        "\n",
        "        sequence_autoencoder.compile(optimizer='adam')\n",
        "\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        x_train = np.transpose(x_train)[:, 0:32]\n",
        "\n",
        "        noise = get_white_noise(x_train.reshape(-1),SNR).reshape(-1,32)\n",
        "        #print(noise)\n",
        "        x_train_noise = np.add(x_train, noise)\n",
        "        x_train = x_train.reshape(-1,1,32)\n",
        "        x_train_noise = x_train_noise.reshape(-1,1,32)\n",
        "\n",
        "        x_test = x_train\n",
        "        x_test_noise = x_train_noise\n",
        "\n",
        "        # print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        print('x_train shape : ',x_train.shape)\n",
        "\n",
        "        sequence_autoencoder.fit(x_train_noise, x_train,\n",
        "                shuffle=True,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                validation_data=(x_test_noise, x_test))\n",
        "\n",
        "        # build a model to project inputs\n",
        "        decoded_x = sequence_autoencoder.predict(x_train)\n",
        "        encoded_x = encoder.predict(x_train)\n",
        "\n",
        "        print('encoded_x : ', encoded_x.shape)\n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/CSEQAE/encoded_eegs_1CSEQae/encoded_eegs_1CSEQae_sub_1d' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        # sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/VAE/encoded_eegs_2vae_z_sub/encoded_eegs_2vae_z_sub' +\n",
        "        #             str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #             {'encoded_eegs_z': encoded_x_z})\n",
        "\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train.reshape(-1)))\n",
        "        #sio.savemat('D:\\\\VAE Experiment\\\\DEAP\\\\decoded_eegs_2vae\\\\decoded_eegs_2vae_sub' +\n",
        "        #            str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "        #            {'decoded_eegs': decoded_x})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuvZlSnQ619S"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=8\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEhq5L1-619T"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/CSEQAE/encoded_eegs_1CSEQae/encoded_eegs_1CSEQae_sub_1d'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        #trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/CSEQAE_ecoded_dataset_8__d.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlyEuiNmvcv_"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKiExIDSMH02"
      },
      "source": [
        "# **Contractive Seq to Seq Model Combined**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXyFLigzMH0_"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from keras.layers import Input, Dense, LeakyReLU, LSTM, RepeatVector\n",
        "from keras.models import Model\n",
        "import scipy.io as sio\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "batch_size = 512\n",
        "original_dim = 32\n",
        "zscore = True\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for subNo in tqdm(range(1,33)): #subNo 从1到32\n",
        "        #### train the VAE on normalized (z-score) multi-channel EEG data\n",
        "        print('subNo: '+str(subNo))\n",
        "        if zscore:\n",
        "            sub_data_file = h5py.File('/content/drive/MyDrive/EMOTION/matlab_data/normalize_zscore/sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['zscore_data']\n",
        "        else:\n",
        "            sub_data_file = h5py.File('D:\\\\Processed DEAP DATA\\\\normalize_minmax\\\\sub' + str(subNo) + '.mat', 'r')\n",
        "            x_train = sub_data_file['minmax_data']\n",
        "\n",
        "        print('shape : ',x_train.shape)\n",
        "        X_train.append(np.transpose(x_train)[:, 0:32])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJuZfNT_PLcD"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrAyun3tPLet"
      },
      "source": [
        "x_train = np.array(X_train).reshape(-1,1,32)\n",
        "x_test = x_train\n",
        "\n",
        "print('x_train shape : ',x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c26pqg_LMoO6"
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "timesteps = 1  # Length of your sequences\n",
        "input_dim = 32 \n",
        "latent_dim = 32\n",
        "lam = 1e-4\n",
        "\n",
        "inputs = Input(shape=(timesteps, input_dim))\n",
        "#print(inputs.shape)\n",
        "encoded = LSTM(latent_dim)(inputs)\n",
        "encoded = Dense(latent_dim, activation='sigmoid', name='encoded')(encoded)\n",
        "#print(encoded.shape)\n",
        "decoded = Dense(input_dim)(encoded)\n",
        "decoded = RepeatVector(timesteps)(decoded)\n",
        "#print(decoded.shape)\n",
        "decoded = LSTM(input_dim, return_sequences=True)(decoded)\n",
        "#print(decoded.shape)\n",
        "sequence_autoencoder = Model(inputs, decoded)\n",
        "encoder = Model(inputs, encoded)\n",
        "\n",
        "\n",
        "mse = K.mean(K.square(inputs - decoded), axis=2)\n",
        "W = K.variable(value=sequence_autoencoder.get_layer('encoded').get_weights()[0])  # N x N_hidden\n",
        "W = K.transpose(W)  # N_hidden x N\n",
        "h = sequence_autoencoder.get_layer('encoded').output\n",
        "\n",
        "dh = h * (1 - h)  # N_batch x N_hidden\n",
        "\n",
        "# N_batch x N_hidden * N_hidden x 1 = N_batch x 1\n",
        "contractive = lam * K.sum(dh**2 * K.sum(W**2, axis=1), axis=1)\n",
        "\n",
        "ae_loss = mse + contractive\n",
        "sequence_autoencoder.add_loss(ae_loss)\n",
        "\n",
        "sequence_autoencoder.compile(optimizer='adam')\n",
        "\n",
        "sequence_autoencoder.fit(x_train, x_train,\n",
        "        shuffle=True,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruYGBN80MwPy"
      },
      "source": [
        "x_train_for_decode = x_train.reshape(32,-1,1,32)\n",
        "for subNo, x_train_ind in tqdm(enumerate(x_train_for_decode)):\n",
        "        # build a model to project inputs\n",
        "        encoded_x = encoder.predict(x_train_ind)\n",
        "        \n",
        "\n",
        "        print('encoded_x shape : ',encoded_x.shape)\n",
        "        \n",
        "     \n",
        "        sio.savemat('/content/drive/MyDrive/EMOTION/matlab_data/CSEQAE/encoded_eegs_1CSEQae/encoded_eegs_1CSEQae_sub' +\n",
        "                    str(subNo) + '_latentdim' + str(latent_dim) + '.mat',\n",
        "                    {'encoded_eegs': encoded_x})\n",
        "        \n",
        "        decoded_x = sequence_autoencoder.predict(x_train_ind)\n",
        "\n",
        "        print('Mean Difference : ', np.mean(decoded_x.reshape(-1)-x_train_ind.reshape(-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMn5l08YMH1B"
      },
      "source": [
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=32\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j7Qd2SPMH1B"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/CSEQAE/encoded_eegs_1CSEQae/encoded_eegs_1CSEQae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/CSEQAE_all_encoded_dataset_32.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0VaUxL7fFko"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ov9T9DeoC_O"
      },
      "source": [
        "# **STORE DATA SEPERATELY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wBxaxzFNFYn"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=16\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "            l_data = pickle.load(filepath)\n",
        "l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXP8qDIsLV30"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pred_test_f1=[]\n",
        "pred_train_f1=[]\n",
        "pred_test_acc=[]\n",
        "pred_train_acc=[]\n",
        "\n",
        "for testSubNo in range(1,33):\n",
        "    X_test = []\n",
        "    X_train = []  # list类型\n",
        "    if testSubNo==1:\n",
        "        Y_test = l_data[0:testSubNo * trialNum, emodim]\n",
        "        Y_train = l_data[testSubNo*trialNum:32*trialNum,emodim]\n",
        "    elif testSubNo==32:\n",
        "        Y_test = l_data[(testSubNo-1)*trialNum:testSubNo * trialNum, emodim]\n",
        "        Y_train = l_data[0:(testSubNo-1)*trialNum, emodim]\n",
        "    else:\n",
        "        Y_test = l_data[(testSubNo - 1) * trialNum:testSubNo * trialNum, emodim]\n",
        "        Y_train1 = l_data[0:(testSubNo - 1) * trialNum, emodim]\n",
        "        Y_train2 = l_data[testSubNo * trialNum:32*trialNum, emodim]\n",
        "        Y_train = np.concatenate((Y_train1,Y_train2))\n",
        "\n",
        "    print('test subNo: '+str(testSubNo))\n",
        "    file1 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/AE/encoded_eegs_1ae/encoded_eegs_1ae_sub'+ str(testSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "\n",
        "    testSubData = file1['encoded_eegs']\n",
        "    testSubData = ZscoreNormalization(testSubData)\n",
        "    #print(t_data.shape)\n",
        "    #构造X_test序列集\n",
        "    for trialNo in range(0,40):\n",
        "        trial_data = testSubData[trialNo*trialL:(trialNo+1)*trialL,:]\n",
        "        #以step等间隔采样\n",
        "        #trial_data = trial_data[0:trialL:step,:]\n",
        "        #print(trial_data.shape)\n",
        "        #将序列插入list，list中每个元素即一个序列\n",
        "        X_test.append(trial_data)\n",
        "    #print(len(X_test))\n",
        "    #print(len(Y_test))\n",
        "\n",
        "    # 构造X_train序列集\n",
        "    for trainSubNo in range(1,33):\n",
        "        if trainSubNo==testSubNo:\n",
        "            continue\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/AE/encoded_eegs_1ae/encoded_eegs_1ae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "    print(len(X_train))\n",
        "    print(len(Y_train))\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    print('shape of X_train : ',X_train.shape)\n",
        "    print('shape of X_test : ',X_test.shape)\n",
        "\n",
        "    with open('/content/drive/MyDrive/EMOTION/encoder/AE_ecoded_dataset/final_video_X_train_60_AE' + str(testSubNo) + '.pkl', 'wb') as filepath:\n",
        "              pickle.dump(X_train[:,-7680:], filepath)\n",
        "    with open('/content/drive/MyDrive/EMOTION/encoder/AE_ecoded_dataset/labels_final_video_y_train_60_AE' + str(testSubNo) + '.pkl', 'wb') as filepath:\n",
        "              pickle.dump(Y_train, filepath)\n",
        "\n",
        "    with open('/content/drive/MyDrive/EMOTION/encoder/AE_ecoded_dataset/final_video_X_test_60_AE' + str(testSubNo) + '.pkl', 'wb') as filepath:\n",
        "              pickle.dump(X_test[:,-7680:], filepath)\n",
        "    with open('/content/drive/MyDrive/EMOTION/encoder/AE_ecoded_dataset/labels_final_video_y_test_60_AE' + str(testSubNo) + '.pkl', 'wb') as filepath:\n",
        "              pickle.dump(Y_test, filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOn2vNYzOJYm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0H_X6dxVQUO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ek8IrpFVQW3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6mNbUBMn9TE"
      },
      "source": [
        "# **STORE ALL DATA IN ONE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs7_meniVQ9u"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=32\n",
        "step=128*3 #控制序列长度\n",
        "method='2vae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=5\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZ9aIsmLVQ9v"
      },
      "source": [
        "def ZscoreNormalization(x):\n",
        "    \"\"\"Z-score normaliaztion\"\"\"\n",
        "    x = (x - np.mean(x)) / np.std(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "X_train = []\n",
        "\n",
        "for trainSubNo in range(1,33):\n",
        "        file2 = sio.loadmat('/content/drive/MyDrive/EMOTION/matlab_data/CAE/encoded_eegs_1Cae/encoded_eegs_1Cae_sub'+ str(trainSubNo) + '_latentdim' + str(latdim) + '.mat')\n",
        "        trainSubData = file2['encoded_eegs']\n",
        "        trainSubData = ZscoreNormalization(trainSubData)\n",
        "        for trialNo in range(0, 40):\n",
        "            trial_data = trainSubData[trialNo * trialL:(trialNo + 1) * trialL, :]\n",
        "            # 以step等间隔采样\n",
        "            #trial_data = trial_data[0:trialL:step, :]\n",
        "            # 将序列插入list，list中每个元素即一个序列\n",
        "            X_train.append(trial_data)\n",
        "        print(len(X_train))\n",
        "print(len(X_train))\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/CAE_ecoded_dataset_32.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OMzhqFdVQZa"
      },
      "source": [
        "np.array(X_train).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G___ytF-VQcL"
      },
      "source": [
        "with open('/content/drive/MyDrive/EMOTION/encoder/AE_ecoded_dataset_128.pkl', 'wb') as filepath:\n",
        "      pickle.dump(np.array(X_train)[:,-7680:], filepath, protocol=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL0nJj40VQeM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQMu6l7n4Y1"
      },
      "source": [
        "# **IND AE TRAIN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXEawVpSh5Fu"
      },
      "source": [
        "from keras.layers  import Input, LSTM, Dense, Dropout, Bidirectional\n",
        "from keras.models import Model, Sequential\n",
        "import h5py\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "import pickle\n",
        "\n",
        "trialL=63*128\n",
        "trialNum=40\n",
        "latdim=16\n",
        "step=128*3 #控制序列长度\n",
        "method='1ae'\n",
        "emodim=1 #valence 0 arousal 1\n",
        "batch=40\n",
        "epoch_num=10\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#             l_data = pickle.load(filepath)\n",
        "# l_data = l_data.reshape(-1,2)\n",
        "# l_data[l_data<5] = 0\n",
        "# l_data[l_data>=5] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIh9bxrEh5Fx"
      },
      "source": [
        "pred_test_f1=[]\n",
        "pred_train_f1=[]\n",
        "pred_test_acc=[]\n",
        "pred_train_acc=[]\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/encoder/AE_ecoded_dataset_32.pkl', 'rb') as filepath:\n",
        "#       x_data = pickle.load(filepath)\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#       y_data = pickle.load(filepath)\n",
        "\n",
        "# x_data = x_data.reshape(-1,7680,32)\n",
        "# y_data = y_data.reshape(-1,2)[:,0]\n",
        "# y_data[y_data<5] = 0\n",
        "# y_data[y_data>=5] = 1\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nz_Seed3_att2.pkl', 'rb') as filepath:\n",
        "          x_data = pickle.load(filepath)\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#           y_data = pickle.load(filepath)\n",
        "# temp = sio.loadmat('/content/drive/MyDrive/EMOTION/Preprocessed_EEG/label.mat')\n",
        "# temp = np.array(temp['label'])[0]\n",
        "temp = np.array([ 1,  1, 0, 0,  1,  1, 0,  1,  1,  1,  1, 0,  1,  1, 0])\n",
        "y_data = np.zeros((15,15))\n",
        "\n",
        "for i in range(15):\n",
        "  y_data[i] = temp\n",
        "\n",
        "y_data = y_data.reshape(-1)\n",
        "\n",
        "x_data = x_data.reshape(-1,30000,16)\n",
        "\n",
        "for testSubNo in range(1,16):\n",
        "    \n",
        "\n",
        "    indd = testSubNo*15\n",
        "\n",
        "    X_train1, X_train2, X_test, y_train1, y_train2, Y_test = x_data[:indd-15], x_data[indd:], x_data[indd-15:indd], y_data[:indd-15], y_data[indd:], y_data[indd-15:indd]\n",
        "    Y_train = np.concatenate((y_train1,y_train2))\n",
        "    X_train = np.concatenate((X_train1,X_train2))\n",
        "\n",
        "    print('test subNo: '+str(testSubNo))\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    print('shape of X_train : ',X_train.shape)\n",
        "    print('shape of X_test : ',X_test.shape)\n",
        "\n",
        "    #构造RNN模型\n",
        "    seqL = X_train.shape[1]\n",
        "    print('seqence length: '+str(seqL))\n",
        "    model = Sequential()\n",
        "    model.add((LSTM(200, input_shape=(seqL, latdim))))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
        "    model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=epoch_num, batch_size=batch)\n",
        "    # predict on the training data after training...\n",
        "    # Y_test_predict = model.predict_classes([X_test])\n",
        "    # Y_train_predict = model.predict_classes([X_train])\n",
        "\n",
        "    # print(metrics.f1_score(Y_test, Y_test_predict))\n",
        "    # print(metrics.accuracy_score(Y_test, Y_test_predict))\n",
        "    # print(metrics.f1_score(Y_train, Y_train_predict))\n",
        "    # print(metrics.accuracy_score(Y_train, Y_train_predict))\n",
        "    # print(classification_report(np.array(Y_test), np.array(Y_test_predict)))\n",
        "\n",
        "    # pred_test_f1.append(metrics.f1_score(Y_test, Y_test_predict))\n",
        "    # pred_test_acc.append(metrics.accuracy_score(Y_test, Y_test_predict))\n",
        "    # pred_train_f1.append(metrics.f1_score(Y_train, Y_train_predict))\n",
        "    # pred_train_acc.append(metrics.accuracy_score(Y_train, Y_train_predict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KiZznsviDL7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NJ2NLshAEb1"
      },
      "source": [
        "# **INTRO**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skbnPvG_nc0b"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "import sklearn\n",
        "import pickle\n",
        "\n",
        "#import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxsxPhIXokVi"
      },
      "source": [
        "from builtins import print\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
        "matplotlib.rcParams['font.sans-serif'] = 'Arial'\n",
        "import os\n",
        "import operator\n",
        "\n",
        "#import utils\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.io import loadmat\n",
        "\n",
        "\n",
        "def readucr(filename):\n",
        "    data = np.loadtxt(filename, delimiter=',')\n",
        "    Y = data[:, 0]\n",
        "    X = data[:, 1:]\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def create_directory(directory_path):\n",
        "    if os.path.exists(directory_path):\n",
        "        return None\n",
        "    else:\n",
        "        try:\n",
        "            os.makedirs(directory_path)\n",
        "        except:\n",
        "            # in case another machine created the path meanwhile !:(\n",
        "            return None\n",
        "        return directory_path\n",
        "\n",
        "\n",
        "def create_path(root_dir, classifier_name, archive_name):\n",
        "    output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '/'\n",
        "    if os.path.exists(output_directory):\n",
        "        return None\n",
        "    else:\n",
        "        os.makedirs(output_directory)\n",
        "        return output_directory\n",
        "\n",
        "\n",
        "def read_dataset(root_dir, archive_name, dataset_name):\n",
        "    datasets_dict = {}\n",
        "    cur_root_dir = root_dir.replace('-temp', '')\n",
        "\n",
        "    if archive_name == 'mts_archive':\n",
        "        file_name = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
        "        x_train = np.load(file_name + 'x_train.npy')\n",
        "        y_train = np.load(file_name + 'y_train.npy')\n",
        "        x_test = np.load(file_name + 'x_test.npy')\n",
        "        y_test = np.load(file_name + 'y_test.npy')\n",
        "\n",
        "        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
        "                                       y_test.copy())\n",
        "\n",
        "    elif archive_name == 'UCRArchive_2018':\n",
        "        root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
        "        df_train = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TRAIN.tsv', sep='\\t', header=None)\n",
        "\n",
        "        df_test = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TEST.tsv', sep='\\t', header=None)\n",
        "\n",
        "        y_train = df_train.values[:, 0]\n",
        "        y_test = df_test.values[:, 0]\n",
        "\n",
        "        x_train = df_train.drop(columns=[0])\n",
        "        x_test = df_test.drop(columns=[0])\n",
        "\n",
        "        x_train.columns = range(x_train.shape[1])\n",
        "        x_test.columns = range(x_test.shape[1])\n",
        "\n",
        "        x_train = x_train.values\n",
        "        x_test = x_test.values\n",
        "\n",
        "        # znorm\n",
        "        std_ = x_train.std(axis=1, keepdims=True)\n",
        "        std_[std_ == 0] = 1.0\n",
        "        x_train = (x_train - x_train.mean(axis=1, keepdims=True)) / std_\n",
        "\n",
        "        std_ = x_test.std(axis=1, keepdims=True)\n",
        "        std_[std_ == 0] = 1.0\n",
        "        x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n",
        "\n",
        "        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
        "                                       y_test.copy())\n",
        "    else:\n",
        "        file_name = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/' + dataset_name\n",
        "        x_train, y_train = readucr(file_name + '_TRAIN')\n",
        "        x_test, y_test = readucr(file_name + '_TEST')\n",
        "        datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
        "                                       y_test.copy())\n",
        "\n",
        "    return datasets_dict\n",
        "\n",
        "\n",
        "def read_all_datasets(root_dir, archive_name, split_val=False):\n",
        "    datasets_dict = {}\n",
        "    cur_root_dir = root_dir.replace('-temp', '')\n",
        "    dataset_names_to_sort = []\n",
        "\n",
        "    if archive_name == 'mts_archive':\n",
        "\n",
        "        for dataset_name in MTS_DATASET_NAMES:\n",
        "            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
        "\n",
        "            x_train = np.load(root_dir_dataset + 'x_train.npy')\n",
        "            y_train = np.load(root_dir_dataset + 'y_train.npy')\n",
        "            x_test = np.load(root_dir_dataset + 'x_test.npy')\n",
        "            y_test = np.load(root_dir_dataset + 'y_test.npy')\n",
        "\n",
        "            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
        "                                           y_test.copy())\n",
        "    elif archive_name == 'UCRArchive_2018':\n",
        "        for dataset_name in DATASET_NAMES_2018:\n",
        "            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
        "\n",
        "            df_train = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TRAIN.tsv', sep='\\t', header=None)\n",
        "\n",
        "            df_test = pd.read_csv(root_dir_dataset + '/' + dataset_name + '_TEST.tsv', sep='\\t', header=None)\n",
        "\n",
        "            y_train = df_train.values[:, 0]\n",
        "            y_test = df_test.values[:, 0]\n",
        "\n",
        "            x_train = df_train.drop(columns=[0])\n",
        "            x_test = df_test.drop(columns=[0])\n",
        "\n",
        "            x_train.columns = range(x_train.shape[1])\n",
        "            x_test.columns = range(x_test.shape[1])\n",
        "\n",
        "            x_train = x_train.values\n",
        "            x_test = x_test.values\n",
        "\n",
        "            # znorm\n",
        "            std_ = x_train.std(axis=1, keepdims=True)\n",
        "            std_[std_ == 0] = 1.0\n",
        "            x_train = (x_train - x_train.mean(axis=1, keepdims=True)) / std_\n",
        "\n",
        "            std_ = x_test.std(axis=1, keepdims=True)\n",
        "            std_[std_ == 0] = 1.0\n",
        "            x_test = (x_test - x_test.mean(axis=1, keepdims=True)) / std_\n",
        "\n",
        "            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
        "                                           y_test.copy())\n",
        "\n",
        "    else:\n",
        "        for dataset_name in DATASET_NAMES:\n",
        "            root_dir_dataset = cur_root_dir + '/archives/' + archive_name + '/' + dataset_name + '/'\n",
        "            file_name = root_dir_dataset + dataset_name\n",
        "            x_train, y_train = readucr(file_name + '_TRAIN')\n",
        "            x_test, y_test = readucr(file_name + '_TEST')\n",
        "\n",
        "            datasets_dict[dataset_name] = (x_train.copy(), y_train.copy(), x_test.copy(),\n",
        "                                           y_test.copy())\n",
        "\n",
        "            dataset_names_to_sort.append((dataset_name, len(x_train)))\n",
        "\n",
        "        dataset_names_to_sort.sort(key=operator.itemgetter(1))\n",
        "\n",
        "        for i in range(len(DATASET_NAMES)):\n",
        "            DATASET_NAMES[i] = dataset_names_to_sort[i][0]\n",
        "\n",
        "    return datasets_dict\n",
        "\n",
        "\n",
        "def get_func_length(x_train, x_test, func):\n",
        "    if func == min:\n",
        "        func_length = np.inf\n",
        "    else:\n",
        "        func_length = 0\n",
        "\n",
        "    n = x_train.shape[0]\n",
        "    for i in range(n):\n",
        "        func_length = func(func_length, x_train[i].shape[1])\n",
        "\n",
        "    n = x_test.shape[0]\n",
        "    for i in range(n):\n",
        "        func_length = func(func_length, x_test[i].shape[1])\n",
        "\n",
        "    return func_length\n",
        "\n",
        "\n",
        "def transform_to_same_length(x, n_var, max_length):\n",
        "    n = x.shape[0]\n",
        "\n",
        "    # the new set in ucr form np array\n",
        "    ucr_x = np.zeros((n, max_length, n_var), dtype=np.float64)\n",
        "\n",
        "    # loop through each time series\n",
        "    for i in range(n):\n",
        "        mts = x[i]\n",
        "        curr_length = mts.shape[1]\n",
        "        idx = np.array(range(curr_length))\n",
        "        idx_new = np.linspace(0, idx.max(), max_length)\n",
        "        for j in range(n_var):\n",
        "            ts = mts[j]\n",
        "            # linear interpolation\n",
        "            f = interp1d(idx, ts, kind='cubic')\n",
        "            new_ts = f(idx_new)\n",
        "            ucr_x[i, :, j] = new_ts\n",
        "\n",
        "    return ucr_x\n",
        "\n",
        "\n",
        "def transform_mts_to_ucr_format():\n",
        "    mts_root_dir = '/mnt/Other/mtsdata/'\n",
        "    mts_out_dir = '/mnt/nfs/casimir/archives/mts_archive/'\n",
        "    for dataset_name in MTS_DATASET_NAMES:\n",
        "        # print('dataset_name',dataset_name)\n",
        "\n",
        "        out_dir = mts_out_dir + dataset_name + '/'\n",
        "\n",
        "        # if create_directory(out_dir) is None:\n",
        "        #     print('Already_done')\n",
        "        #     continue\n",
        "\n",
        "        a = loadmat(mts_root_dir + dataset_name + '/' + dataset_name + '.mat')\n",
        "        a = a['mts']\n",
        "        a = a[0, 0]\n",
        "\n",
        "        dt = a.dtype.names\n",
        "        dt = list(dt)\n",
        "\n",
        "        for i in range(len(dt)):\n",
        "            if dt[i] == 'train':\n",
        "                x_train = a[i].reshape(max(a[i].shape))\n",
        "            elif dt[i] == 'test':\n",
        "                x_test = a[i].reshape(max(a[i].shape))\n",
        "            elif dt[i] == 'trainlabels':\n",
        "                y_train = a[i].reshape(max(a[i].shape))\n",
        "            elif dt[i] == 'testlabels':\n",
        "                y_test = a[i].reshape(max(a[i].shape))\n",
        "\n",
        "        # x_train = a[1][0]\n",
        "        # y_train = a[0][:,0]\n",
        "        # x_test = a[3][0]\n",
        "        # y_test = a[2][:,0]\n",
        "\n",
        "        n_var = x_train[0].shape[0]\n",
        "\n",
        "        max_length = get_func_length(x_train, x_test, func=max)\n",
        "        min_length = get_func_length(x_train, x_test, func=min)\n",
        "\n",
        "        print(dataset_name, 'max', max_length, 'min', min_length)\n",
        "        print()\n",
        "        # continue\n",
        "\n",
        "        x_train = transform_to_same_length(x_train, n_var, max_length)\n",
        "        x_test = transform_to_same_length(x_test, n_var, max_length)\n",
        "\n",
        "        # save them\n",
        "        np.save(out_dir + 'x_train.npy', x_train)\n",
        "        np.save(out_dir + 'y_train.npy', y_train)\n",
        "        np.save(out_dir + 'x_test.npy', x_test)\n",
        "        np.save(out_dir + 'y_test.npy', y_test)\n",
        "\n",
        "        print('Done')\n",
        "\n",
        "\n",
        "def calculate_metrics(y_true, y_pred, duration, y_true_val=None, y_pred_val=None):\n",
        "    res = pd.DataFrame(data=np.zeros((1, 4), dtype=np.float), index=[0],\n",
        "                       columns=['precision', 'accuracy', 'recall', 'duration'])\n",
        "    res['precision'] = precision_score(y_true, y_pred, average='macro')\n",
        "    res['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    if not y_true_val is None:\n",
        "        # this is useful when transfer learning is used with cross validation\n",
        "        res['accuracy_val'] = accuracy_score(y_true_val, y_pred_val)\n",
        "\n",
        "    res['recall'] = recall_score(y_true, y_pred, average='macro')\n",
        "    res['duration'] = duration\n",
        "    return res\n",
        "\n",
        "\n",
        "def save_test_duration(file_name, test_duration):\n",
        "    res = pd.DataFrame(data=np.zeros((1, 1), dtype=np.float), index=[0],\n",
        "                       columns=['test_duration'])\n",
        "    res['test_duration'] = test_duration\n",
        "    res.to_csv(file_name, index=False)\n",
        "\n",
        "\n",
        "def generate_results_csv(output_file_name, root_dir):\n",
        "    res = pd.DataFrame(data=np.zeros((0, 7), dtype=np.float), index=[],\n",
        "                       columns=['classifier_name', 'archive_name', 'dataset_name',\n",
        "                                'precision', 'accuracy', 'recall', 'duration'])\n",
        "    for classifier_name in CLASSIFIERS:\n",
        "        for archive_name in ARCHIVE_NAMES:\n",
        "            datasets_dict = read_all_datasets(root_dir, archive_name)\n",
        "            for it in range(ITERATIONS):\n",
        "                curr_archive_name = archive_name\n",
        "                if it != 0:\n",
        "                    curr_archive_name = curr_archive_name + '_itr_' + str(it)\n",
        "                for dataset_name in datasets_dict.keys():\n",
        "                    output_dir = root_dir + '/results/' + classifier_name + '/' \\\n",
        "                                 + curr_archive_name + '/' + dataset_name + '/' + 'df_metrics.csv'\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        continue\n",
        "                    df_metrics = pd.read_csv(output_dir)\n",
        "                    df_metrics['classifier_name'] = classifier_name\n",
        "                    df_metrics['archive_name'] = archive_name\n",
        "                    df_metrics['dataset_name'] = dataset_name\n",
        "                    res = pd.concat((res, df_metrics), axis=0, sort=False)\n",
        "\n",
        "    res.to_csv(root_dir + output_file_name, index=False)\n",
        "    # aggreagte the accuracy for iterations on same dataset\n",
        "    res = pd.DataFrame({\n",
        "        'accuracy': res.groupby(\n",
        "            ['classifier_name', 'archive_name', 'dataset_name'])['accuracy'].mean()\n",
        "    }).reset_index()\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "def plot_epochs_metric(hist, file_name, metric='loss'):\n",
        "    plt.figure()\n",
        "    plt.plot(hist.history[metric])\n",
        "    plt.plot(hist.history['val_' + metric])\n",
        "    plt.title('model ' + metric)\n",
        "    plt.ylabel(metric, fontsize='large')\n",
        "    plt.xlabel('epoch', fontsize='large')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.savefig(file_name, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_logs_t_leNet(output_directory, hist, y_pred, y_true, duration):\n",
        "    hist_df = pd.DataFrame(hist.history)\n",
        "    hist_df.to_csv(output_directory + 'history.csv', index=False)\n",
        "\n",
        "    df_metrics = calculate_metrics(y_true, y_pred, duration)\n",
        "    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n",
        "\n",
        "    index_best_model = hist_df['loss'].idxmin()\n",
        "    row_best_model = hist_df.loc[index_best_model]\n",
        "\n",
        "    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float), index=[0],\n",
        "                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n",
        "                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n",
        "\n",
        "    df_best_model['best_model_train_loss'] = row_best_model['loss']\n",
        "    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n",
        "    df_best_model['best_model_train_acc'] = row_best_model['acc']\n",
        "    df_best_model['best_model_val_acc'] = row_best_model['val_acc']\n",
        "    df_best_model['best_model_nb_epoch'] = index_best_model\n",
        "\n",
        "    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n",
        "\n",
        "    # plot losses\n",
        "    plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n",
        "\n",
        "\n",
        "def save_logs(output_directory, hist, y_pred, y_true, duration, lr=True, y_true_val=None, y_pred_val=None):\n",
        "    hist_df = pd.DataFrame(hist.history)\n",
        "    hist_df.to_csv(output_directory + 'history.csv', index=False)\n",
        "\n",
        "    df_metrics = calculate_metrics(y_true, y_pred, duration, y_true_val, y_pred_val)\n",
        "    df_metrics.to_csv(output_directory + 'df_metrics.csv', index=False)\n",
        "\n",
        "    index_best_model = hist_df['loss'].idxmin()\n",
        "    row_best_model = hist_df.loc[index_best_model]\n",
        "\n",
        "    df_best_model = pd.DataFrame(data=np.zeros((1, 6), dtype=np.float), index=[0],\n",
        "                                 columns=['best_model_train_loss', 'best_model_val_loss', 'best_model_train_acc',\n",
        "                                          'best_model_val_acc', 'best_model_learning_rate', 'best_model_nb_epoch'])\n",
        "\n",
        "    df_best_model['best_model_train_loss'] = row_best_model['loss']\n",
        "    df_best_model['best_model_val_loss'] = row_best_model['val_loss']\n",
        "    df_best_model['best_model_train_acc'] = row_best_model['accuracy']\n",
        "    df_best_model['best_model_val_acc'] = row_best_model['val_accuracy']\n",
        "    if lr == True:\n",
        "        df_best_model['best_model_learning_rate'] = row_best_model['lr']\n",
        "    df_best_model['best_model_nb_epoch'] = index_best_model\n",
        "\n",
        "    df_best_model.to_csv(output_directory + 'df_best_model.csv', index=False)\n",
        "\n",
        "    # for FCN there is no hyperparameters fine tuning - everything is static in code\n",
        "\n",
        "    # plot losses\n",
        "    plot_epochs_metric(hist, output_directory + 'epochs_loss.png')\n",
        "\n",
        "    return df_metrics\n",
        "\n",
        "\n",
        "def visualize_filter(root_dir):\n",
        "    import tensorflow.keras as keras\n",
        "    classifier = 'resnet'\n",
        "    archive_name = 'UCRArchive_2018'\n",
        "    dataset_name = 'GunPoint'\n",
        "    datasets_dict = read_dataset(root_dir, archive_name, dataset_name)\n",
        "\n",
        "    x_train = datasets_dict[dataset_name][0]\n",
        "    y_train = datasets_dict[dataset_name][1]\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "\n",
        "    model = keras.models.load_model(\n",
        "        root_dir + 'results/' + classifier + '/' + archive_name + '/' + dataset_name + '/best_model.hdf5')\n",
        "\n",
        "    # filters\n",
        "    filters = model.layers[1].get_weights()[0]\n",
        "\n",
        "    new_input_layer = model.inputs\n",
        "    new_output_layer = [model.layers[1].output]\n",
        "\n",
        "    new_feed_forward = keras.backend.function(new_input_layer, new_output_layer)\n",
        "\n",
        "    classes = np.unique(y_train)\n",
        "\n",
        "    colors = [(255 / 255, 160 / 255, 14 / 255), (181 / 255, 87 / 255, 181 / 255)]\n",
        "    colors_conv = [(210 / 255, 0 / 255, 0 / 255), (27 / 255, 32 / 255, 101 / 255)]\n",
        "\n",
        "    idx = 10\n",
        "    idx_filter = 1\n",
        "\n",
        "    filter = filters[:, 0, idx_filter]\n",
        "\n",
        "    plt.figure(1)\n",
        "    plt.plot(filter + 0.5, color='gray', label='filter')\n",
        "    for c in classes:\n",
        "        c_x_train = x_train[np.where(y_train == c)]\n",
        "        convolved_filter_1 = new_feed_forward([c_x_train])[0]\n",
        "\n",
        "        idx_c = int(c) - 1\n",
        "\n",
        "        plt.plot(c_x_train[idx], color=colors[idx_c], label='class' + str(idx_c) + '-raw')\n",
        "        plt.plot(convolved_filter_1[idx, :, idx_filter], color=colors_conv[idx_c], label='class' + str(idx_c) + '-conv')\n",
        "        plt.legend()\n",
        "\n",
        "    plt.savefig(root_dir + 'convolution-' + dataset_name + '.pdf')\n",
        "\n",
        "    return 1\n",
        "\n",
        "\n",
        "def viz_perf_themes(root_dir, df):\n",
        "    df_themes = df.copy()\n",
        "    themes_index = []\n",
        "    # add the themes\n",
        "    for dataset_name in df.index:\n",
        "        themes_index.append(utils.constants.dataset_types[dataset_name])\n",
        "\n",
        "    themes_index = np.array(themes_index)\n",
        "    themes, themes_counts = np.unique(themes_index, return_counts=True)\n",
        "    df_themes.index = themes_index\n",
        "    df_themes = df_themes.rank(axis=1, method='min', ascending=False)\n",
        "    df_themes = df_themes.where(df_themes.values == 1)\n",
        "    df_themes = df_themes.groupby(level=0).sum(axis=1)\n",
        "    df_themes['#'] = themes_counts\n",
        "\n",
        "    for classifier in CLASSIFIERS:\n",
        "        df_themes[classifier] = df_themes[classifier] / df_themes['#'] * 100\n",
        "    df_themes = df_themes.round(decimals=1)\n",
        "    df_themes.to_csv(root_dir + 'tab-perf-theme.csv')\n",
        "\n",
        "\n",
        "def viz_perf_train_size(root_dir, df):\n",
        "    df_size = df.copy()\n",
        "    train_sizes = []\n",
        "    datasets_dict_ucr = read_all_datasets(root_dir, archive_name='UCR_TS_Archive_2015')\n",
        "    datasets_dict_mts = read_all_datasets(root_dir, archive_name='mts_archive')\n",
        "    datasets_dict = dict(datasets_dict_ucr, **datasets_dict_mts)\n",
        "\n",
        "    for dataset_name in df.index:\n",
        "        train_size = len(datasets_dict[dataset_name][0])\n",
        "        train_sizes.append(train_size)\n",
        "\n",
        "    train_sizes = np.array(train_sizes)\n",
        "    bins = np.array([0, 100, 400, 800, 99999])\n",
        "    train_size_index = np.digitize(train_sizes, bins)\n",
        "    train_size_index = bins[train_size_index]\n",
        "\n",
        "    df_size.index = train_size_index\n",
        "    df_size = df_size.rank(axis=1, method='min', ascending=False)\n",
        "    df_size = df_size.groupby(level=0, axis=0).mean()\n",
        "    df_size = df_size.round(decimals=2)\n",
        "\n",
        "    print(df_size.to_string())\n",
        "    df_size.to_csv(root_dir + 'tab-perf-train-size.csv')\n",
        "\n",
        "\n",
        "def viz_perf_classes(root_dir, df):\n",
        "    df_classes = df.copy()\n",
        "    class_numbers = []\n",
        "    datasets_dict_ucr = read_all_datasets(root_dir, archive_name='UCR_TS_Archive_2015')\n",
        "    datasets_dict_mts = read_all_datasets(root_dir, archive_name='mts_archive')\n",
        "    datasets_dict = dict(datasets_dict_ucr, **datasets_dict_mts)\n",
        "\n",
        "    for dataset_name in df.index:\n",
        "        train_size = len(np.unique(datasets_dict[dataset_name][1]))\n",
        "        class_numbers.append(train_size)\n",
        "\n",
        "    class_numbers = np.array(class_numbers)\n",
        "    bins = np.array([0, 3, 4, 6, 8, 13, 9999])\n",
        "    class_numbers_index = np.digitize(class_numbers, bins)\n",
        "    class_numbers_index = bins[class_numbers_index]\n",
        "\n",
        "    df_classes.index = class_numbers_index\n",
        "    df_classes = df_classes.rank(axis=1, method='min', ascending=False)\n",
        "    df_classes = df_classes.groupby(level=0, axis=0).mean()\n",
        "    df_classes = df_classes.round(decimals=2)\n",
        "\n",
        "    print(df_classes.to_string())\n",
        "    df_classes.to_csv(root_dir + 'tab-perf-classes.csv')\n",
        "\n",
        "\n",
        "def viz_perf_length(root_dir, df):\n",
        "    df_lengths = df.copy()\n",
        "    lengths = []\n",
        "    datasets_dict_ucr = read_all_datasets(root_dir, archive_name='UCR_TS_Archive_2015')\n",
        "    datasets_dict_mts = read_all_datasets(root_dir, archive_name='mts_archive')\n",
        "    datasets_dict = dict(datasets_dict_ucr, **datasets_dict_mts)\n",
        "\n",
        "    for dataset_name in df.index:\n",
        "        length = datasets_dict[dataset_name][0].shape[1]\n",
        "        lengths.append(length)\n",
        "\n",
        "    lengths = np.array(lengths)\n",
        "    bins = np.array([0, 81, 251, 451, 700, 1001, 9999])\n",
        "    lengths_index = np.digitize(lengths, bins)\n",
        "    lengths_index = bins[lengths_index]\n",
        "\n",
        "    df_lengths.index = lengths_index\n",
        "    df_lengths = df_lengths.rank(axis=1, method='min', ascending=False)\n",
        "    df_lengths = df_lengths.groupby(level=0, axis=0).mean()\n",
        "    df_lengths = df_lengths.round(decimals=2)\n",
        "\n",
        "    print(df_lengths.to_string())\n",
        "    df_lengths.to_csv(root_dir + 'tab-perf-lengths.csv')\n",
        "\n",
        "\n",
        "def viz_plot(root_dir, df):\n",
        "    df_lengths = df.copy()\n",
        "    lengths = []\n",
        "    datasets_dict_ucr = read_all_datasets(root_dir, archive_name='UCR_TS_Archive_2015')\n",
        "    datasets_dict_mts = read_all_datasets(root_dir, archive_name='mts_archive')\n",
        "    datasets_dict = dict(datasets_dict_ucr, **datasets_dict_mts)\n",
        "\n",
        "    for dataset_name in df.index:\n",
        "        length = datasets_dict[dataset_name][0].shape[1]\n",
        "        lengths.append(length)\n",
        "\n",
        "    lengths_index = np.array(lengths)\n",
        "\n",
        "    df_lengths.index = lengths_index\n",
        "\n",
        "    plt.scatter(x=df_lengths['fcn'], y=df_lengths['resnet'])\n",
        "    plt.ylim(ymin=0, ymax=1.05)\n",
        "    plt.xlim(xmin=0, xmax=1.05)\n",
        "    # df_lengths['fcn']\n",
        "    plt.savefig(root_dir + 'plot.pdf')\n",
        "\n",
        "\n",
        "def viz_for_survey_paper(root_dir, filename='results-ucr-mts.csv'):\n",
        "    df = pd.read_csv(root_dir + filename, index_col=0)\n",
        "    df = df.T\n",
        "    df = df.round(decimals=2)\n",
        "\n",
        "    # get table performance per themes\n",
        "    # viz_perf_themes(root_dir,df)\n",
        "\n",
        "    # get table performance per train size\n",
        "    # viz_perf_train_size(root_dir,df)\n",
        "\n",
        "    # get table performance per classes\n",
        "    # viz_perf_classes(root_dir,df)\n",
        "\n",
        "    # get table performance per length\n",
        "    # viz_perf_length(root_dir,df)\n",
        "\n",
        "    # get plot\n",
        "    viz_plot(root_dir, df)\n",
        "\n",
        "\n",
        "def viz_cam(root_dir):\n",
        "    import tensorflow.keras as keras\n",
        "    import sklearn\n",
        "    classifier = 'resnet'\n",
        "    archive_name = 'UCRArchive_2018'\n",
        "    dataset_name = 'GunPoint'\n",
        "\n",
        "    if dataset_name == 'Gun_Point':\n",
        "        save_name = 'GunPoint'\n",
        "    else:\n",
        "        save_name = dataset_name\n",
        "    max_length = 2000\n",
        "    datasets_dict = read_dataset(root_dir, archive_name, dataset_name)\n",
        "\n",
        "    x_train = datasets_dict[dataset_name][0]\n",
        "    y_train = datasets_dict[dataset_name][1]\n",
        "    y_test = datasets_dict[dataset_name][3]\n",
        "\n",
        "    # transform to binary labels\n",
        "    enc = sklearn.preprocessing.OneHotEncoder()\n",
        "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
        "    y_train_binary = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
        "\n",
        "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
        "\n",
        "    model = keras.models.load_model(\n",
        "        root_dir + 'results/' + classifier + '/' + archive_name + '/' + dataset_name + '/best_model.hdf5')\n",
        "\n",
        "    # filters\n",
        "    w_k_c = model.layers[-1].get_weights()[0]  # weights for each filter k for each class c\n",
        "\n",
        "    # the same input\n",
        "    new_input_layer = model.inputs\n",
        "    # output is both the original as well as the before last layer\n",
        "    new_output_layer = [model.layers[-3].output, model.layers[-1].output]\n",
        "\n",
        "    new_feed_forward = keras.backend.function(new_input_layer, new_output_layer)\n",
        "\n",
        "    classes = np.unique(y_train)\n",
        "\n",
        "    for c in classes:\n",
        "        plt.figure()\n",
        "        count = 0\n",
        "        c_x_train = x_train[np.where(y_train == c)]\n",
        "        for ts in c_x_train:\n",
        "            ts = ts.reshape(1, -1, 1)\n",
        "            [conv_out, predicted] = new_feed_forward([ts])\n",
        "            pred_label = np.argmax(predicted)\n",
        "            orig_label = np.argmax(enc.transform([[c]]))\n",
        "            if pred_label == orig_label:\n",
        "                cas = np.zeros(dtype=np.float, shape=(conv_out.shape[1]))\n",
        "                for k, w in enumerate(w_k_c[:, orig_label]):\n",
        "                    cas += w * conv_out[0, :, k]\n",
        "\n",
        "                minimum = np.min(cas)\n",
        "\n",
        "                cas = cas - minimum\n",
        "\n",
        "                cas = cas / max(cas)\n",
        "                cas = cas * 100\n",
        "\n",
        "                x = np.linspace(0, ts.shape[1] - 1, max_length, endpoint=True)\n",
        "                # linear interpolation to smooth\n",
        "                f = interp1d(range(ts.shape[1]), ts[0, :, 0])\n",
        "                y = f(x)\n",
        "                # if (y < -2.2).any():\n",
        "                #     continue\n",
        "                f = interp1d(range(ts.shape[1]), cas)\n",
        "                cas = f(x).astype(int)\n",
        "                plt.scatter(x=x, y=y, c=cas, cmap='jet', marker='.', s=2, vmin=0, vmax=100, linewidths=0.0)\n",
        "                if dataset_name == 'Gun_Point':\n",
        "                    if c == 1:\n",
        "                        plt.yticks([-1.0, 0.0, 1.0, 2.0])\n",
        "                    else:\n",
        "                        plt.yticks([-2, -1.0, 0.0, 1.0, 2.0])\n",
        "                count += 1\n",
        "\n",
        "        cbar = plt.colorbar()\n",
        "        # cbar.ax.set_yticklabels([100,75,50,25,0])\n",
        "        plt.savefig(root_dir + '/temp/' + classifier + '-cam-' + save_name + '-class-' + str(int(c)) + '.png',\n",
        "                    bbox_inches='tight', dpi=1080)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsL8LKfGotSn"
      },
      "source": [
        "UNIVARIATE_DATASET_NAMES = ['50words', 'Adiac', 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken', 'Car', 'CBF',\n",
        "                            'ChlorineConcentration', 'CinC_ECG_torso', 'Coffee',\n",
        "                            'Computers', 'Cricket_X', 'Cricket_Y', 'Cricket_Z', 'DiatomSizeReduction',\n",
        "                            'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW',\n",
        "                            'Earthquakes', 'ECG200', 'ECG5000', 'ECGFiveDays', 'ElectricDevices', 'FaceAll', 'FaceFour',\n",
        "                            'FacesUCR', 'FISH', 'FordA', 'FordB', 'Gun_Point', 'Ham', 'HandOutlines',\n",
        "                            'Haptics', 'Herring', 'InlineSkate', 'InsectWingbeatSound', 'ItalyPowerDemand',\n",
        "                            'LargeKitchenAppliances', 'Lighting2', 'Lighting7', 'MALLAT', 'Meat', 'MedicalImages',\n",
        "                            'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect', 'MiddlePhalanxTW',\n",
        "                            'MoteStrain', 'NonInvasiveFatalECG_Thorax1', 'NonInvasiveFatalECG_Thorax2', 'OliveOil',\n",
        "                            'OSULeaf', 'PhalangesOutlinesCorrect', 'Phoneme', 'Plane', 'ProximalPhalanxOutlineAgeGroup',\n",
        "                            'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW', 'RefrigerationDevices',\n",
        "                            'ScreenType', 'ShapeletSim', 'ShapesAll', 'SmallKitchenAppliances', 'SonyAIBORobotSurface',\n",
        "                            'SonyAIBORobotSurfaceII', 'StarLightCurves', 'Strawberry', 'SwedishLeaf', 'Symbols',\n",
        "                            'synthetic_control', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace', 'TwoLeadECG',\n",
        "                            'Two_Patterns', 'UWaveGestureLibraryAll', 'uWaveGestureLibrary_X', 'uWaveGestureLibrary_Y',\n",
        "                            'uWaveGestureLibrary_Z', 'wafer', 'Wine', 'WordsSynonyms', 'Worms', 'WormsTwoClass', 'yoga']\n",
        "\n",
        "UNIVARIATE_DATASET_NAMES_2018 = ['ACSF1', 'Adiac', 'AllGestureWiimoteX', 'AllGestureWiimoteY', 'AllGestureWiimoteZ',\n",
        "                                 'ArrowHead', 'Beef', 'BeetleFly', 'BirdChicken', 'BME', 'Car', 'CBF', 'Chinatown',\n",
        "                                 'ChlorineConcentration', 'CinCECGTorso', 'Coffee', 'Computers', 'CricketX',\n",
        "                                 'CricketY', 'CricketZ', 'Crop', 'DiatomSizeReduction',\n",
        "                                 'DistalPhalanxOutlineAgeGroup', 'DistalPhalanxOutlineCorrect', 'DistalPhalanxTW',\n",
        "                                 'DodgerLoopDay', 'DodgerLoopGame', 'DodgerLoopWeekend', 'Earthquakes', 'ECG200',\n",
        "                                 'ECG5000', 'ECGFiveDays', 'ElectricDevices', 'EOGHorizontalSignal',\n",
        "                                 'EOGVerticalSignal', 'EthanolLevel', 'FaceAll', 'FaceFour', 'FacesUCR',\n",
        "                                 'FiftyWords', 'Fish', 'FordA', 'FordB', 'FreezerRegularTrain',\n",
        "                                 'FreezerSmallTrain', 'Fungi', 'GestureMidAirD1', 'GestureMidAirD2',\n",
        "                                 'GestureMidAirD3', 'GesturePebbleZ1', 'GesturePebbleZ2', 'GunPoint',\n",
        "                                 'GunPointAgeSpan', 'GunPointMaleVersusFemale', 'GunPointOldVersusYoung',\n",
        "                                 'Ham', 'HandOutlines', 'Haptics', 'Herring', 'HouseTwenty', 'InlineSkate',\n",
        "                                 'InsectEPGRegularTrain', 'InsectEPGSmallTrain', 'InsectWingbeatSound',\n",
        "                                 'ItalyPowerDemand', 'LargeKitchenAppliances', 'Lightning2', 'Lightning7',\n",
        "                                 'Mallat', 'Meat', 'MedicalImages', 'MelbournePedestrian',\n",
        "                                 'MiddlePhalanxOutlineAgeGroup', 'MiddlePhalanxOutlineCorrect',\n",
        "                                 'MiddlePhalanxTW', 'MixedShapesRegularTrain', 'MixedShapesSmallTrain',\n",
        "                                 'MoteStrain', 'NonInvasiveFetalECGThorax1', 'NonInvasiveFetalECGThorax2',\n",
        "                                 'OliveOil', 'OSULeaf', 'PhalangesOutlinesCorrect', 'Phoneme',\n",
        "                                 'PickupGestureWiimoteZ', 'PigAirwayPressure', 'PigArtPressure', 'PigCVP',\n",
        "                                 'PLAID', 'Plane', 'PowerCons', 'ProximalPhalanxOutlineAgeGroup',\n",
        "                                 'ProximalPhalanxOutlineCorrect', 'ProximalPhalanxTW', 'RefrigerationDevices',\n",
        "                                 'Rock', 'ScreenType', 'SemgHandGenderCh2', 'SemgHandMovementCh2',\n",
        "                                 'SemgHandSubjectCh2', 'ShakeGestureWiimoteZ', 'ShapeletSim', 'ShapesAll',\n",
        "                                 'SmallKitchenAppliances', 'SmoothSubspace', 'SonyAIBORobotSurface1',\n",
        "                                 'SonyAIBORobotSurface2', 'StarLightCurves', 'Strawberry', 'SwedishLeaf',\n",
        "                                 'Symbols', 'SyntheticControl', 'ToeSegmentation1', 'ToeSegmentation2', 'Trace',\n",
        "                                 'TwoLeadECG', 'TwoPatterns', 'UMD', 'UWaveGestureLibraryAll',\n",
        "                                 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibraryZ',\n",
        "                                 'Wafer', 'Wine', 'WordSynonyms', 'Worms', 'WormsTwoClass', 'Yoga']\n",
        "\n",
        "MTS_DATASET_NAMES = ['ArabicDigits', 'AUSLAN', 'CharacterTrajectories', 'CMUsubject16', 'ECG',\n",
        "                     'JapaneseVowels', 'KickvsPunch', 'Libras', 'NetFlow', 'UWave', 'Wafer', 'WalkvsRun']\n",
        "\n",
        "ITERATIONS = 5  # nb of random runs for random initializations\n",
        "\n",
        "ARCHIVE_NAMES = ['UCRArchive_2018']\n",
        "\n",
        "dataset_names_for_archive = {'UCRArchive_2018': UNIVARIATE_DATASET_NAMES_2018}\n",
        "\n",
        "CLASSIFIERS = ['fcn', 'mlp', 'resnet', 'tlenet', 'mcnn', 'twiesn', 'encoder', 'mcdcnn', 'cnn', 'inception']\n",
        "\n",
        "dataset_types = {'ElectricDevices': 'DEVICE', 'FordB': 'SENSOR',\n",
        "                 'FordA': 'SENSOR', 'NonInvasiveFatalECG_Thorax2': 'ECG',\n",
        "                 'NonInvasiveFatalECG_Thorax1': 'ECG', 'PhalangesOutlinesCorrect': 'IMAGE',\n",
        "                 'HandOutlines': 'IMAGE', 'StarLightCurves': 'SENSOR',\n",
        "                 'wafer': 'SENSOR', 'Two_Patterns': 'SIMULATED',\n",
        "                 'UWaveGestureLibraryAll': 'MOTION', 'uWaveGestureLibrary_Z': 'MOTION',\n",
        "                 'uWaveGestureLibrary_Y': 'MOTION', 'uWaveGestureLibrary_X': 'MOTION',\n",
        "                 'Strawberry': 'SPECTRO', 'ShapesAll': 'IMAGE',\n",
        "                 'ProximalPhalanxOutlineCorrect': 'IMAGE', 'MiddlePhalanxOutlineCorrect': 'IMAGE',\n",
        "                 'DistalPhalanxOutlineCorrect': 'IMAGE', 'FaceAll': 'IMAGE',\n",
        "                 'ECG5000': 'ECG', 'SwedishLeaf': 'IMAGE', 'ChlorineConcentration': 'SIMULATED',\n",
        "                 '50words': 'IMAGE', 'ProximalPhalanxTW': 'IMAGE', 'ProximalPhalanxOutlineAgeGroup': 'IMAGE',\n",
        "                 'MiddlePhalanxOutlineAgeGroup': 'IMAGE', 'DistalPhalanxTW': 'IMAGE',\n",
        "                 'DistalPhalanxOutlineAgeGroup': 'IMAGE', 'MiddlePhalanxTW': 'IMAGE',\n",
        "                 'Cricket_Z': 'MOTION', 'Cricket_Y': 'MOTION',\n",
        "                 'Cricket_X': 'MOTION', 'Adiac': 'IMAGE',\n",
        "                 'MedicalImages': 'IMAGE', 'SmallKitchenAppliances': 'DEVICE',\n",
        "                 'ScreenType': 'DEVICE', 'RefrigerationDevices': 'DEVICE',\n",
        "                 'LargeKitchenAppliances': 'DEVICE', 'Earthquakes': 'SENSOR',\n",
        "                 'yoga': 'IMAGE', 'synthetic_control': 'SIMULATED',\n",
        "                 'WordsSynonyms': 'IMAGE', 'Computers': 'DEVICE',\n",
        "                 'InsectWingbeatSound': 'SENSOR', 'Phoneme': 'SENSOR',\n",
        "                 'OSULeaf': 'IMAGE', 'FacesUCR': 'IMAGE',\n",
        "                 'WormsTwoClass': 'MOTION', 'Worms': 'MOTION',\n",
        "                 'FISH': 'IMAGE', 'Haptics': 'MOTION',\n",
        "                 'Epilepsy': 'HAR', 'Ham': 'SPECTRO',\n",
        "                 'Plane': 'SENSOR', 'InlineSkate': 'MOTION',\n",
        "                 'Trace': 'SENSOR', 'ECG200': 'ECG',\n",
        "                 'Lighting7': 'SENSOR', 'ItalyPowerDemand': 'SENSOR',\n",
        "                 'Herring': 'IMAGE', 'Lighting2': 'SENSOR',\n",
        "                 'Car': 'SENSOR', 'Meat': 'SPECTRO',\n",
        "                 'Wine': 'SPECTRO', 'MALLAT': 'SIMULATED',\n",
        "                 'Gun_Point': 'MOTION', 'CinC_ECG_torso': 'ECG',\n",
        "                 'ToeSegmentation1': 'MOTION', 'ToeSegmentation2': 'MOTION',\n",
        "                 'ArrowHead': 'IMAGE', 'OliveOil': 'SPECTRO',\n",
        "                 'Beef': 'SPECTRO', 'CBF': 'SIMULATED',\n",
        "                 'Coffee': 'SPECTRO', 'SonyAIBORobotSurfaceII': 'SENSOR',\n",
        "                 'Symbols': 'IMAGE', 'FaceFour': 'IMAGE',\n",
        "                 'ECGFiveDays': 'ECG', 'TwoLeadECG': 'ECG',\n",
        "                 'BirdChicken': 'IMAGE', 'BeetleFly': 'IMAGE',\n",
        "                 'ShapeletSim': 'SIMULATED', 'MoteStrain': 'SENSOR',\n",
        "                 'SonyAIBORobotSurface': 'SENSOR', 'DiatomSizeReduction': 'IMAGE'}\n",
        "\n",
        "themes_colors = {'IMAGE': 'red', 'SENSOR': 'blue', 'ECG': 'green',\n",
        "                 'SIMULATED': 'yellow', 'SPECTRO': 'orange',\n",
        "                 'MOTION': 'purple', 'DEVICE': 'gray'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQd2Z4GozTI"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "class Classifier_INCEPTION:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, batch_size=64, lr=0.001,\n",
        "                 nb_filters=32, use_residual=True, use_bottleneck=True, depth=6, kernel_size=41, nb_epochs=1500):\n",
        "\n",
        "        self.output_directory = output_directory\n",
        "\n",
        "        self.nb_filters = nb_filters\n",
        "        self.use_residual = use_residual\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.depth = depth\n",
        "        self.kernel_size = kernel_size - 1\n",
        "        self.callbacks = None\n",
        "        self.batch_size = batch_size\n",
        "        self.bottleneck_size = 32\n",
        "        self.nb_epochs = nb_epochs\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose\n",
        "\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "\n",
        "    def _inception_module(self, input_tensor, stride=1, activation='linear'):\n",
        "\n",
        "        if self.use_bottleneck and int(input_tensor.shape[-1]) > self.bottleneck_size:\n",
        "            input_inception = keras.layers.Conv1D(filters=self.bottleneck_size, kernel_size=1,\n",
        "                                                  padding='same', activation=activation, use_bias=False)(input_tensor)\n",
        "        else:\n",
        "            input_inception = input_tensor\n",
        "\n",
        "        # kernel_size_s = [3, 5, 8, 11, 17]\n",
        "        kernel_size_s = [self.kernel_size // (2 ** i) for i in range(3)]\n",
        "\n",
        "        conv_list = []\n",
        "\n",
        "        for i in range(len(kernel_size_s)):\n",
        "            conv_list.append(keras.layers.Conv1D(filters=self.nb_filters, kernel_size=kernel_size_s[i],\n",
        "                                                 strides=stride, padding='same', activation=activation, use_bias=False)(\n",
        "                input_inception))\n",
        "\n",
        "        max_pool_1 = keras.layers.MaxPool1D(pool_size=3, strides=stride, padding='same')(input_tensor)\n",
        "\n",
        "        conv_6 = keras.layers.Conv1D(filters=self.nb_filters, kernel_size=1,\n",
        "                                     padding='same', activation=activation, use_bias=False)(max_pool_1)\n",
        "\n",
        "        conv_list.append(conv_6)\n",
        "\n",
        "        x = keras.layers.Concatenate(axis=2)(conv_list)\n",
        "        x = keras.layers.BatchNormalization()(x)\n",
        "        x = keras.layers.Activation(activation='relu')(x)\n",
        "        return x\n",
        "\n",
        "    def _shortcut_layer(self, input_tensor, out_tensor):\n",
        "        shortcut_y = keras.layers.Conv1D(filters=int(out_tensor.shape[-1]), kernel_size=1,\n",
        "                                         padding='same', use_bias=False)(input_tensor)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        x = keras.layers.Add()([shortcut_y, out_tensor])\n",
        "        x = keras.layers.Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        x = input_layer\n",
        "        input_res = input_layer\n",
        "\n",
        "        for d in range(self.depth):\n",
        "\n",
        "            x = self._inception_module(x)\n",
        "\n",
        "            if self.use_residual and d % 3 == 2:\n",
        "                x = self._shortcut_layer(input_res, x)\n",
        "                input_res = x\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(self.lr),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
        "                                                      min_lr=0.0001)\n",
        "\n",
        "        file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "                                                           save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, batch_size):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error no gpu')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        if self.batch_size is None:\n",
        "            mini_batch_size = int(min(x_train.shape[0] / 10, 16))\n",
        "        else:\n",
        "            mini_batch_size = self.batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=self.nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
        "                              return_df_metrics=False)\n",
        "\n",
        "        # save predictions\n",
        "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
        "\n",
        "        # convert the predicted from binary to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return df_metrics\n",
        "\n",
        "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
        "        start_time = time.time()\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test, batch_size=self.batch_size)\n",
        "        if return_df_metrics:\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
        "            return df_metrics\n",
        "        else:\n",
        "            test_duration = time.time() - start_time\n",
        "            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
        "            return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9wNQKJvo8wE"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Classifier_RESNET:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False, build=True, load_weights=False):\n",
        "        self.output_directory = output_directory\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            if load_weights == True:\n",
        "                self.model.load_weights(self.output_directory\n",
        "                                        .replace('resnet_augment', 'resnet')\n",
        "                                        .replace('TSC_itr_augment_x_10', 'TSC_itr_10')\n",
        "                                        + '/model_init.hdf5')\n",
        "            else:\n",
        "                self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "        return\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        n_feature_maps = 64\n",
        "\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # BLOCK 1\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=8, padding='same')(input_layer)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps, kernel_size=1, padding='same')(input_layer)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_1 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_1 = keras.layers.Activation('relu')(output_block_1)\n",
        "\n",
        "        # BLOCK 2\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_1)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # expand channels for the sum\n",
        "        shortcut_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=1, padding='same')(output_block_1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "\n",
        "        output_block_2 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_2 = keras.layers.Activation('relu')(output_block_2)\n",
        "\n",
        "        # BLOCK 3\n",
        "\n",
        "        conv_x = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=8, padding='same')(output_block_2)\n",
        "        conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "        conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "\n",
        "        conv_y = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=5, padding='same')(conv_x)\n",
        "        conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "        conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "\n",
        "        conv_z = keras.layers.Conv1D(filters=n_feature_maps * 2, kernel_size=3, padding='same')(conv_y)\n",
        "        conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "        # no need to expand channels because they are equal\n",
        "        shortcut_y = keras.layers.BatchNormalization()(output_block_2)\n",
        "\n",
        "        output_block_3 = keras.layers.add([shortcut_y, conv_z])\n",
        "        output_block_3 = keras.layers.Activation('relu')(output_block_3)\n",
        "\n",
        "        # FINAL\n",
        "\n",
        "        gap_layer = keras.layers.GlobalAveragePooling1D()(output_block_3)\n",
        "\n",
        "        output_layer = keras.layers.Dense(nb_classes, activation='softmax')(gap_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50, min_lr=0.0001)\n",
        "\n",
        "        file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "        model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, monitor='loss',\n",
        "                                                           save_best_only=True)\n",
        "\n",
        "        self.callbacks = [reduce_lr, model_checkpoint]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, batch_size):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "        batch_size = batch_size\n",
        "        nb_epochs = 10\n",
        "\n",
        "        mini_batch_size = int(min(x_train.shape[0] / 10, batch_size))\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        self.model.save(self.output_directory + 'last_model.hdf5')\n",
        "\n",
        "        y_pred = self.predict(x_val, y_true, x_train, y_train, y_val,\n",
        "                              return_df_metrics=False)\n",
        "\n",
        "        # save predictions\n",
        "        np.save(self.output_directory + 'y_pred.npy', y_pred)\n",
        "\n",
        "        # convert the predicted from binary to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "        df_metrics = save_logs(self.output_directory, hist, y_pred, y_true, duration)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "\n",
        "        return df_metrics\n",
        "\n",
        "    def predict(self, x_test, y_true, x_train, y_train, y_test, return_df_metrics=True):\n",
        "        start_time = time.time()\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "        if return_df_metrics:\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
        "            return df_metrics\n",
        "        else:\n",
        "            test_duration = time.time() - start_time\n",
        "            save_test_duration(self.output_directory + 'test_duration.csv', test_duration)\n",
        "            return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb8yghiXfYI1"
      },
      "source": [
        "!pip install tensorflow_addons"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT3ULr0w19X0"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import time\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "#from utils.utils import save_logs\n",
        "#from utils.utils import calculate_metrics\n",
        "\n",
        "class Classifier_ENCODER:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False,build=True):\n",
        "        self.output_directory = output_directory\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            #self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        # # conv block -1\n",
        "        # conv1 = keras.layers.Conv1D(filters=32,kernel_size=3,strides=1,padding='same')(input_layer)\n",
        "        # conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "        # conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "        # conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "        # conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "        # # conv block -2\n",
        "        # conv1 = keras.layers.Conv1D(filters=64,kernel_size=3,strides=1,padding='same')(conv1)\n",
        "        # conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "        # conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "        # conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "        # conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "        # conv block -3\n",
        "        conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(input_layer)\n",
        "        conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "        conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "        conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "        conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "        # conv block -4\n",
        "        conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(conv1)\n",
        "        conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "        conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "        conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
        "        conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "        # conv block -5\n",
        "        conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv2)\n",
        "        conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "        conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "        conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "        # conv block -6\n",
        "        # conv3 = keras.layers.Conv1D(filters=1024,kernel_size=21,strides=1,padding='same')(conv3)\n",
        "        # conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "        # conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "        # conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "        # split for attention\n",
        "        attention_data = keras.layers.Lambda(lambda x: x[:,:,:256])(conv3)\n",
        "        attention_softmax = keras.layers.Lambda(lambda x: x[:,:,256:])(conv3)\n",
        "        # attention mechanism\n",
        "        attention_softmax = keras.layers.Softmax(name='attention_vec')(attention_softmax)\n",
        "        multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
        "        # last layer\n",
        "        dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
        "        dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "        # output layer\n",
        "        flatten_layer = keras.layers.Flatten()(dense_layer)\n",
        "        output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(flatten_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.00001),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "        # model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path,\n",
        "        #                                                    monitor='loss', save_best_only=True)\n",
        "\n",
        "        es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
        "        #self.callbacks = [model_checkpoint,es]\n",
        "        self.callbacks = [es]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "        batch_size = 64\n",
        "        nb_epochs = 50\n",
        "\n",
        "        mini_batch_size = batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        #self.model.save(self.output_directory+'last_model.hdf5')\n",
        "\n",
        "        #model = keras.models.load_model(self.output_directory + 'best_model.hdf5')\n",
        "\n",
        "        model = self.model\n",
        "\n",
        "        y_pred = model.predict(x_val)\n",
        "        print('binary y_pred : ',y_pred)\n",
        "\n",
        "        # convert the predicted from binary to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        print('argmaxed y_pred : ',y_pred)\n",
        "        #print(classification_report(np.array(y_val), np.array(y_pred)))\n",
        "\n",
        "        #save_logs(self.output_directory, hist, y_pred, y_true, duration, lr=False)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "        # return model\n",
        "\n",
        "    def predict(self, x_test,y_true,x_train,y_train,y_test,return_df_metrics = True):\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "        if return_df_metrics:\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
        "            return df_metrics\n",
        "        else:\n",
        "            return y_pred\n",
        "#© 2021 GitHub, Inc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQWIS1tpR_kL"
      },
      "source": [
        "# import tensorflow.keras as keras\n",
        "# import tensorflow as tf\n",
        "# import tensorflow_addons as tfa\n",
        "# import numpy as np\n",
        "# import time\n",
        "\n",
        "# input_layer = keras.layers.Input((7680,8))\n",
        "\n",
        "\n",
        "# conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(input_layer)\n",
        "# conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "# conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "# conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "# conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "# # conv block -4\n",
        "# conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(input_layer)\n",
        "# conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "# conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "# conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
        "# conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "# # conv block -5\n",
        "# conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(input_layer)\n",
        "# conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "# conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "# conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "# conv3 = keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
        "\n",
        "# concat = tf.keras.layers.concatenate([conv1, conv2, conv3], axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSo66UIgzktb"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Dense, Activation, Multiply, Add, Lambda\n",
        "from keras.initializers import Constant\n",
        "\n",
        "\n",
        "class Highway(Layer):\n",
        "\n",
        "    activation = None\n",
        "    transform_gate_bias = None\n",
        "\n",
        "    def __init__(self, activation='relu', transform_gate_bias=-1, **kwargs):\n",
        "        self.activation = activation\n",
        "        self.transform_gate_bias = transform_gate_bias\n",
        "        super(Highway, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        dim = input_shape[-1]\n",
        "        transform_gate_bias_initializer = Constant(self.transform_gate_bias)\n",
        "        input_shape_dense_1 = input_shape[-1]\n",
        "        self.dense_1 = Dense(units=dim, bias_initializer=transform_gate_bias_initializer)\n",
        "        self.dense_1.build(input_shape)\n",
        "        self.dense_2 = Dense(units=dim)\n",
        "        self.dense_2.build(input_shape)\n",
        "        self.trainable_weightssss = self.dense_1.trainable_weights + self.dense_2.trainable_weights\n",
        "\n",
        "        super(Highway, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, x):\n",
        "        dim = K.int_shape(x)[-1]\n",
        "        transform_gate = self.dense_1(x)\n",
        "        transform_gate = Activation(\"sigmoid\")(transform_gate)\n",
        "        carry_gate = Lambda(lambda x: 1.0 - x, output_shape=(dim,))(transform_gate)\n",
        "        transformed_data = self.dense_2(x)\n",
        "        transformed_data = Activation(self.activation)(transformed_data)\n",
        "        transformed_gated = Multiply()([transform_gate, transformed_data])\n",
        "        identity_gated = Multiply()([carry_gate, x])\n",
        "        value = Add()([transformed_gated, identity_gated])\n",
        "        return value\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['activation'] = self.activation\n",
        "        config['transform_gate_bias'] = self.transform_gate_bias\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCBitVUdd2Sw"
      },
      "source": [
        "import tensorflow.keras as keras\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "#from utils.utils import save_logs\n",
        "#from utils.utils import calculate_metrics\n",
        "\n",
        "class Classifier_ENCODER22:\n",
        "\n",
        "    def __init__(self, output_directory, input_shape, nb_classes, verbose=False,build=True):\n",
        "        self.output_directory = output_directory\n",
        "        if build == True:\n",
        "            self.model = self.build_model(input_shape, nb_classes)\n",
        "            if (verbose == True):\n",
        "                self.model.summary()\n",
        "            self.verbose = verbose\n",
        "            #self.model.save_weights(self.output_directory + 'model_init.hdf5')\n",
        "\n",
        "    def build_model(self, input_shape, nb_classes):\n",
        "        input_layer = keras.layers.Input(input_shape)\n",
        "\n",
        "        \n",
        "        # conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "        conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='same')(input_layer)\n",
        "        conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "        conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "        conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "        conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "        # conv block -4\n",
        "        conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='same')(input_layer)\n",
        "        conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "        conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "        conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
        "        # conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "        conv2 = keras.layers.Conv1D(filters=512,kernel_size=11,strides=1,padding='same')(conv2)\n",
        "        conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "        conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "        conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
        "        conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "        \n",
        "        # conv block -5\n",
        "        # conv3 = keras.layers.Conv1D(filters=128,kernel_size=21,strides=1,padding='same')(input_layer)\n",
        "        # conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "        # conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "        # conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "        # # conv3 = keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
        "        # conv3 = keras.layers.Conv1D(filters=256,kernel_size=21,strides=1,padding='same')(conv3)\n",
        "        # conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "        # conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "        # conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "        # conv3 = keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
        "        # # conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='same')(conv3)\n",
        "        # conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "        # conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "        # conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "        # conv3 = keras.layers.MaxPooling1D(pool_size=2)(conv3)\n",
        "\n",
        "        concat_r = tf.keras.layers.concatenate([conv1, conv2], axis=-1)\n",
        "        highway_layer = Highway()(concat_r)\n",
        "        highway_layer = tfa.layers.InstanceNormalization()(highway_layer)\n",
        "        highway_layer = keras.layers.Dropout(rate=0.3)(highway_layer)\n",
        "        highway_layer = keras.layers.MaxPooling1D(pool_size=2)(highway_layer)\n",
        "        \n",
        "        attention_data = keras.layers.Lambda(lambda x: x[:,:,:320])(highway_layer)\n",
        "        attention_softmax = keras.layers.Lambda(lambda x: x[:,:,320:])(highway_layer)\n",
        "        # attention mechanism\n",
        "        attention_softmax = keras.layers.Softmax(name='attention_vec')(attention_softmax)\n",
        "        multiply_layer = keras.layers.Multiply()([attention_softmax,attention_data])\n",
        "        # last layer\n",
        "        dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(multiply_layer)\n",
        "        dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "        # output layer\n",
        "        flatten_layer = keras.layers.Flatten()(dense_layer)\n",
        "        output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(flatten_layer)\n",
        "\n",
        "        model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(0.00001),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        file_path = self.output_directory + 'best_model.hdf5'\n",
        "\n",
        "        # model_checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path,\n",
        "        #                                                    monitor='loss', save_best_only=True)\n",
        "\n",
        "        es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "        #self.callbacks = [model_checkpoint,es]\n",
        "        self.callbacks = [es]\n",
        "\n",
        "        return model\n",
        "\n",
        "    def fit(self, x_train, y_train, x_val, y_val, y_true, batch_size):\n",
        "        if not tf.test.is_gpu_available:\n",
        "            print('error')\n",
        "            exit()\n",
        "        # x_val and y_val are only used to monitor the test loss and NOT for training\n",
        "        batch_size = batch_size\n",
        "        nb_epochs = 50\n",
        "\n",
        "        mini_batch_size = batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
        "                              verbose=self.verbose, validation_data=(x_val, y_val), callbacks=self.callbacks)\n",
        "\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        #self.model.save(self.output_directory+'last_model.hdf5')\n",
        "\n",
        "        #model = keras.models.load_model(self.output_directory + 'best_model.hdf5')\n",
        "\n",
        "        model = self.model\n",
        "\n",
        "        y_pred = model.predict(x_val)\n",
        "        print('binary y_pred : ',y_pred)\n",
        "\n",
        "        # convert the predicted from binary to integer\n",
        "        y_pred = np.argmax(y_pred, axis=1)\n",
        "        print('argmaxed y_pred : ',y_pred)\n",
        "        #print(classification_report(np.array(y_val), np.array(y_pred)))\n",
        "\n",
        "        #save_logs(self.output_directory, hist, y_pred, y_true, duration, lr=False)\n",
        "\n",
        "        keras.backend.clear_session()\n",
        "        # return model\n",
        "\n",
        "    def predict(self, x_test,y_true,x_train,y_train,y_test,return_df_metrics = True):\n",
        "        model_path = self.output_directory + 'best_model.hdf5'\n",
        "        model = keras.models.load_model(model_path)\n",
        "        y_pred = model.predict(x_test)\n",
        "        if return_df_metrics:\n",
        "            y_pred = np.argmax(y_pred, axis=1)\n",
        "            df_metrics = calculate_metrics(y_true, y_pred, 0.0)\n",
        "            return df_metrics\n",
        "        else:\n",
        "            return y_pred\n",
        "#© 2021 GitHub, Inc."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwQjqW_gU1EU"
      },
      "source": [
        "\n",
        "\n",
        "# # keras.layers.MultiHeadAttention()\n",
        "\n",
        "# input_layer = keras.layers.Input((7680,8))\n",
        "\n",
        "# # conv block -1\n",
        "# conv1 = keras.layers.Conv1D(filters=128,kernel_size=5,strides=1,padding='causal')(input_layer)\n",
        "# conv1 = tfa.layers.InstanceNormalization()(conv1)\n",
        "# conv1 = keras.layers.PReLU(shared_axes=[1])(conv1)\n",
        "# conv1 = keras.layers.Dropout(rate=0.2)(conv1)\n",
        "# conv1 = keras.layers.MaxPooling1D(pool_size=2)(conv1)\n",
        "# # conv block -2\n",
        "# conv2 = keras.layers.Conv1D(filters=256,kernel_size=11,strides=1,padding='causal')(conv1)\n",
        "# conv2 = tfa.layers.InstanceNormalization()(conv2)\n",
        "# conv2 = keras.layers.PReLU(shared_axes=[1])(conv2)\n",
        "# conv2 = keras.layers.Dropout(rate=0.2)(conv2)\n",
        "# conv2 = keras.layers.MaxPooling1D(pool_size=2)(conv2)\n",
        "# # conv block -3\n",
        "# conv3 = keras.layers.Conv1D(filters=512,kernel_size=21,strides=1,padding='causal')(conv2)\n",
        "# conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "# conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "# conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n",
        "# # conv block -4\n",
        "# conv3 = keras.layers.Conv1D(filters=1024,kernel_size=21,strides=1,padding='causal')(conv3)\n",
        "# conv3 = tfa.layers.InstanceNormalization()(conv3)\n",
        "# conv3 = keras.layers.PReLU(shared_axes=[1])(conv3)\n",
        "# conv3 = keras.layers.Dropout(rate=0.2)(conv3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzVWPJIEWB6L"
      },
      "source": [
        "# conv3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIgkTeqiVUgQ"
      },
      "source": [
        "# # split for attention\n",
        "# attn_layer = keras.layers.MultiHeadAttention(num_heads=4,key_dim=64, attention_axes=(1, 2))(conv3,conv3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV06jp4_WAha"
      },
      "source": [
        "# attn_layer.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmhwjWe0VVAn"
      },
      "source": [
        "# # last attn_layer\n",
        "# dense_layer = keras.layers.Dense(units=256,activation='sigmoid')(attn_layer)\n",
        "# dense_layer = tfa.layers.InstanceNormalization()(dense_layer)\n",
        "# # output layer\n",
        "# flatten_layer = keras.layers.Flatten()(dense_layer)\n",
        "# output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(flatten_layer)\n",
        "\n",
        "# model = keras.models.Model(inputs=input_layer, outputs=output_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB7qL1JLpGIj"
      },
      "source": [
        "def fit_classifier(x_train, x_test, y_train, y_test, batch_size):\n",
        "    #x_train = datasets_dict[dataset_name][0]\n",
        "    #y_train = datasets_dict[dataset_name][1]\n",
        "    #x_test = datasets_dict[dataset_name][2]\n",
        "    #y_test = datasets_dict[dataset_name][3]\n",
        "\n",
        "    nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
        "\n",
        "    # transform the labels from integers to one hot vectors\n",
        "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "    enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
        "    y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
        "    y_test = enc.transform(y_test.reshape(-1, 1)).toarray()\n",
        "\n",
        "    # save orignal y because later we will use binary\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    if len(x_train.shape) == 2:  # if univariate\n",
        "        # add a dimension to make it multivariate with one dimension \n",
        "        x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
        "        x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
        "\n",
        "    input_shape = x_train.shape[1:]\n",
        "    classifier = create_classifier(classifier_name, input_shape, nb_classes)\n",
        "    print('classifier_name : ',classifier_name)\n",
        "\n",
        "    classifier.fit(x_train, y_train, x_test, y_test, y_true, batch_size)\n",
        "\n",
        "\n",
        "def create_classifier(classifier_name, input_shape, nb_classes, output_directory = '', verbose=True):\n",
        "    if classifier_name == 'fcn':\n",
        "        from classifiers import fcn\n",
        "        return fcn.Classifier_FCN(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'mlp':\n",
        "        from classifiers import mlp\n",
        "        return mlp.Classifier_MLP(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'resnet':\n",
        "        #from classifiers import resnet\n",
        "        return Classifier_RESNET(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'mcnn':\n",
        "        from classifiers import mcnn\n",
        "        return mcnn.Classifier_MCNN(output_directory, verbose)\n",
        "    if classifier_name == 'tlenet':\n",
        "        from classifiers import tlenet\n",
        "        return tlenet.Classifier_TLENET(output_directory, verbose)\n",
        "    if classifier_name == 'twiesn':\n",
        "        from classifiers import twiesn\n",
        "        return twiesn.Classifier_TWIESN(output_directory, verbose)\n",
        "    if classifier_name == 'encoder':\n",
        "        #from classifiers import encoder\n",
        "        return Classifier_ENCODER(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'encoder_lstm':\n",
        "        #from classifiers import encoder\n",
        "        return Classifier_ENCODER22(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'cnn':  # Time-CNN\n",
        "        from classifiers import cnn\n",
        "        return cnn.Classifier_CNN(output_directory, input_shape, nb_classes, verbose)\n",
        "    if classifier_name == 'inception':\n",
        "        #from classifiers import inception\n",
        "        return Classifier_INCEPTION(output_directory, input_shape, nb_classes, verbose)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWZ6IxPBoVR_"
      },
      "source": [
        "# **Denoising Seq AE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FJE6TX6L-5D"
      },
      "source": [
        "import scipy.io as sio\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG5XcYaLl0R3"
      },
      "source": [
        "## DEAP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv6xL6zMn5Fk"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_ICA.pkl', 'rb') as filepath:\n",
        "#           x_data = pickle.load(filepath)\n",
        "# with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nzz7680_attn444__l.pkl', 'rb') as filepath:\n",
        "#           x_data = pickle.load(filepath)\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nzz7680_attn444__bino_all2.pkl', 'rb') as filepath:\n",
        "          x_data = pickle.load(filepath)\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "          y_data = pickle.load(filepath)\n",
        "\n",
        "x_data = x_data.reshape(-1,8064,8)[:,-7680:]#.transpose(0,2,1)\n",
        "y_data = y_data.reshape(-1,2)[:,1]\n",
        "\n",
        "y_data[y_data<5] = 0\n",
        "y_data[y_data>=5] = 1\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "\n",
        "for testSubNo in tqdm(range(1,33)):\n",
        "  ############################################### main\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "  indd = testSubNo*40\n",
        "\n",
        "  X_train1, X_train2, X_test, y_train1, y_train2, y_test = x_data[:indd-40], x_data[indd:], x_data[indd-40:indd], y_data[:indd-40], y_data[indd:], y_data[indd-40:indd]\n",
        "  y_train = np.concatenate((y_train1,y_train2))\n",
        "  X_train = np.concatenate((X_train1,X_train2))\n",
        "\n",
        "  print('X_train : ',X_train.shape)\n",
        "  print('y_train : ',y_train.shape)\n",
        "  print('X_test : ',X_test.shape)\n",
        "  print('y_test : ',y_test.shape)\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  # X_train = X_train.reshape(-1,768,32)\n",
        "  # X_test = X_test.reshape(-1,768,32)\n",
        "\n",
        "  # y_train_final = np.zeros(10*len(y_train))\n",
        "  # count = 0\n",
        "\n",
        "  # for label in y_train:\n",
        "  #   for i in range(10):\n",
        "  #     y_train_final[count] = label\n",
        "  #     count += 1\n",
        "\n",
        "  # y_test_final = np.zeros(10*len(y_test))\n",
        "  # count = 0\n",
        "\n",
        "  # for label in y_test:\n",
        "  #   for i in range(10):\n",
        "  #     y_test_final[count] = label\n",
        "  #     count += 1\n",
        "\n",
        "  # y_train_final = y_train\n",
        "  # y_test_final = y_test\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "\n",
        "  print('test subNo: '+str(testSubNo))\n",
        "\n",
        "  # change this directory for your machine\n",
        "  #root_dir = '/b/home/uha/hfawaz-datas/dl-tsc-temp/'\n",
        "\n",
        "  classifier_name = 'encoder_lstm'\n",
        "  output_directory = ''\n",
        "\n",
        "  fit_classifier(X_train, X_test, y_train, y_test, batch_size)\n",
        "\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTo6du2Zl766"
      },
      "source": [
        "## Seizure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjP6402Imse5"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse_5sss_attnn__1ll.pkl', 'rb') as filepath:\n",
        "          x_data = pickle.load(filepath)\n",
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/y_data_ss_ip.pkl', 'rb') as filepath:\n",
        "      y_data = pickle.load(filepath)\n",
        "\n",
        "x_data = x_data.reshape(-1,5120,8)#[:,-7680:]#.transpose(0,2,1)\n",
        "y_data = y_data.reshape(-1)\n",
        "\n",
        "# y_data[y_data<5] = 0\n",
        "# y_data[y_data>=5] = 1\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "for testSubNo in tqdm(range(1,10)):\n",
        "  ############################################### main\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "  indd = testSubNo*18\n",
        "\n",
        "  X_train1, X_train2, X_test, y_train1, y_train2, y_test = x_data[:indd-18], x_data[indd:], x_data[indd-18:indd], y_data[:indd-18], y_data[indd:], y_data[indd-18:indd]\n",
        "  y_train = np.concatenate((y_train1,y_train2))\n",
        "  X_train = np.concatenate((X_train1,X_train2))\n",
        "\n",
        "  print('X_train : ',X_train.shape)\n",
        "  print('y_train : ',y_train.shape)\n",
        "  print('X_test : ',X_test.shape)\n",
        "  print('y_test : ',y_test.shape)\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  # X_train = X_train.reshape(-1,768,32)\n",
        "  # X_test = X_test.reshape(-1,768,32)\n",
        "\n",
        "  # y_train_final = np.zeros(10*len(y_train))\n",
        "  # count = 0\n",
        "\n",
        "  # for label in y_train:\n",
        "  #   for i in range(10):\n",
        "  #     y_train_final[count] = label\n",
        "  #     count += 1\n",
        "\n",
        "  # y_test_final = np.zeros(10*len(y_test))\n",
        "  # count = 0\n",
        "\n",
        "  # for label in y_test:\n",
        "  #   for i in range(10):\n",
        "  #     y_test_final[count] = label\n",
        "  #     count += 1\n",
        "\n",
        "  # y_train_final = y_train\n",
        "  # y_test_final = y_test\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "\n",
        "  print('test subNo: '+str(testSubNo))\n",
        "\n",
        "  # change this directory for your machine\n",
        "  #root_dir = '/b/home/uha/hfawaz-datas/dl-tsc-temp/'\n",
        "\n",
        "  classifier_name = 'encoder_lstm'\n",
        "  output_directory = ''\n",
        "\n",
        "  fit_classifier(X_train, X_test, y_train, y_test, batch_size)\n",
        "\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeyojOK6aBJ_"
      },
      "source": [
        "with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/y_data_ss_ip.pkl', 'rb') as filepath:\n",
        "      y_data = pickle.load(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "788JzyIRZ5Da"
      },
      "source": [
        "y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvueqvyzl_h0"
      },
      "source": [
        "## SEED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rslExY3KoUk_"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nz.pkl', 'rb') as filepath:\n",
        "with open('/content/drive/MyDrive/EMOTION/encoder/DSEQAE_ecoded_dataset_mse88_nz_Seed3_att2.pkl', 'rb') as filepath:\n",
        "          x_data = pickle.load(filepath)\n",
        "# with open('/content/drive/MyDrive/EMOTION/VQVAE-trans/Trans-checkpoints-py/labels_val_arousal.pkl', 'rb') as filepath:\n",
        "#           y_data = pickle.load(filepath)\n",
        "# temp = sio.loadmat('/content/drive/MyDrive/EMOTION/Preprocessed_EEG/label.mat')\n",
        "# temp = np.array(temp['label'])[0]\n",
        "temp = np.array([ 1,  1, 0, 0,  1,  1, 0,  1,  1,  1,  1, 0,  1,  1, 0])\n",
        "y_data = np.zeros((15,15))\n",
        "\n",
        "for i in range(15):\n",
        "  y_data[i] = temp\n",
        "\n",
        "y_data = y_data.reshape(-1)\n",
        "\n",
        "x_data = x_data.reshape(-1,30000,16)#.transpose(0,2,1)\n",
        "# y_data = y_data.reshape(-1,2)[:,1]\n",
        "\n",
        "\n",
        "for testSubNo in tqdm(range(1,16)):\n",
        "  ############################################### main\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "\n",
        "\n",
        "  indd = testSubNo*(15)\n",
        "\n",
        "  X_train1, X_train2, X_test, y_train1, y_train2, y_test = x_data[:indd-(15)], x_data[indd:], x_data[indd-(15):indd], y_data[:indd-(15)], y_data[indd:], y_data[indd-(15):indd]\n",
        "  y_train = np.concatenate((y_train1,y_train2))\n",
        "  X_train = np.concatenate((X_train1,X_train2))\n",
        "\n",
        "  print('X_train : ',X_train.shape)\n",
        "  print('y_train : ',y_train.shape)\n",
        "  print('X_test : ',X_test.shape)\n",
        "  print('y_test : ',y_test.shape)\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  # X_train = X_train.reshape(-1,768,32)\n",
        "  # X_test = X_test.reshape(-1,768,32)\n",
        "\n",
        "  # y_train_final = np.zeros(10*len(y_train))\n",
        "  # count = 0\n",
        "\n",
        "  # for label in y_train:\n",
        "  #   for i in range(10):\n",
        "  #     y_train_final[count] = label\n",
        "  #     count += 1\n",
        "\n",
        "  # y_test_final = np.zeros(10*len(y_test))\n",
        "  # count = 0\n",
        "\n",
        "  # for label in y_test:\n",
        "  #   for i in range(10):\n",
        "  #     y_test_final[count] = label\n",
        "  #     count += 1\n",
        "\n",
        "  # y_train_final = y_train\n",
        "  # y_test_final = y_test\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "  ################################################################################\n",
        "\n",
        "  print('test subNo: '+str(testSubNo))\n",
        "\n",
        "  # change this directory for your machine\n",
        "  #root_dir = '/b/home/uha/hfawaz-datas/dl-tsc-temp/'\n",
        "\n",
        "  classifier_name = 'encoder_lstm'\n",
        "  output_directory = ''\n",
        "\n",
        "  fit_classifier(X_train, X_test, y_train, y_test, 8)\n",
        "\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')\n",
        "  print('-----------------------------------------------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Gr8zArBzgm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FopApJwKgmyG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}